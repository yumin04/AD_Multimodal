{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66140b64",
   "metadata": {},
   "source": [
    "# Tensor Fusion Network w/ Decomposition & Cox Survival Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133cde4f",
   "metadata": {},
   "source": [
    "## Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a521263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import sys\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import time\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import partial_tucker\n",
    "from decompositions import cp_decomposition_conv_layer, tucker_decomposition_conv_layer\n",
    "\n",
    "from VBMF import VBMF\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os, os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import asarray\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sksurv.datasets import load_gbsg2\n",
    "# from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_censored, concordance_index_ipcw, integrated_brier_score\n",
    "\n",
    "import itertools\n",
    "from itertools import *\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import torchtuples as tt\n",
    "from pycox.models import CoxCC\n",
    "from pycox.utils import kaplan_meier\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from ptflops import get_model_complexity_info\n",
    "import torchprofile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f434b",
   "metadata": {},
   "source": [
    "#### Apply Configuration Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de53f8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45210/131645128.py:1: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  torch.autograd.detect_anomaly(True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.detect_anomaly at 0x7fe7e8118730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc183eab",
   "metadata": {},
   "source": [
    "## Determine Compute Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b76196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if possible\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ^ Usually cuda:0, but at time of writing all avaliable memory on GPU 0 is in use.\n",
    "\n",
    "# Use PyTorch as Tensorly Backend\n",
    "tl.set_backend('pytorch')\n",
    "\n",
    "# Force CPU Evaluation (Not Recommended)\n",
    "# device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04619be",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044aee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_ranks(input_tensor):\n",
    "    \"\"\" Unfold the 3 modes of the 3D tensor and estimate ranks using VBMF for each mode \"\"\"\n",
    "    \n",
    "    # Squeeze to remove the first singleton dimension\n",
    "    input_tensor = input_tensor.squeeze(0)  \n",
    "#     print(input_tensor)\n",
    "    \n",
    "    input_tensor[input_tensor == 0] = 1e-6  # This keeps zeros from causing numerical instability\n",
    "    \n",
    "    print(\"Input Tensor Shape:\", input_tensor.size())\n",
    "    print(\"Any NaNs in tensor:\", torch.isnan(input_tensor).any())\n",
    "    print(\"Any zeros in tensor:\", (input_tensor == 0).any())\n",
    "    \n",
    "    # Unfold the tensor along each mode (0, 1, and 2)\n",
    "    unfold_0 = tl.base.unfold(input_tensor, 0)  # Unfold along mode-0\n",
    "    unfold_1 = tl.base.unfold(input_tensor, 1)  # Unfold along mode-1\n",
    "    unfold_2 = tl.base.unfold(input_tensor, 2)  # Unfold along mode-2\n",
    "    \n",
    "    # Add a small epsilon to ensure numerical stability\n",
    "    epsilon = 1e-8\n",
    "    unfold_0 += epsilon\n",
    "    unfold_1 += epsilon\n",
    "    unfold_2 += epsilon\n",
    "    \n",
    "    # Apply VBMF to estimate the ranks for each unfolded matrix\n",
    "    _, diag_0, _, _ = VBMF.EVBMF(unfold_0)\n",
    "    _, diag_1, _, _ = VBMF.EVBMF(unfold_1)\n",
    "    _, diag_2, _, _ = VBMF.EVBMF(unfold_2)\n",
    "    \n",
    "    # The ranks are the number of singular values (or latent dimensions) estimated for each mode\n",
    "    ranks = [diag_0.shape[0], diag_1.shape[1], diag_2.shape[1]]\n",
    "    return ranks\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def calculate_flop(net):\n",
    "    macs, params = get_model_complexity_info(net, (3, 224, 224), as_strings=True, backend='aten', \\\n",
    "                                             print_per_layer_stat=False, verbose=False)\n",
    "    flops = macs[:-5]\n",
    "    flops = float(flops)\n",
    "    flops = flops * 2\n",
    "    \n",
    "    flops = str(flops)\n",
    "    flops = flops + \" FLOP\"\n",
    "    \n",
    "    print('{:<30}  {:<8}'.format('Computational Complexity: ', macs))\n",
    "    print('{:<30}  {:<8}'.format('Approximate FLOP: ', flops))\n",
    "    # print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "    \n",
    "    return macs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b36ca",
   "metadata": {},
   "source": [
    "# Declare TFN Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98a7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOENCODER_INPUTS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a8221",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9bb727",
   "metadata": {},
   "source": [
    "### Deserialize AD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f73f5a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTID</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>035_S_0204</td>\n",
       "      <td>patients_csv/035_S_0204.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>027_S_0256</td>\n",
       "      <td>patients_csv/027_S_0256.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114_S_0173</td>\n",
       "      <td>patients_csv/114_S_0173.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>094_S_0921</td>\n",
       "      <td>patients_csv/094_S_0921.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>007_S_1222</td>\n",
       "      <td>patients_csv/007_S_1222.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>127_S_1032</td>\n",
       "      <td>patients_csv/127_S_1032.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>057_S_0474</td>\n",
       "      <td>patients_csv/057_S_0474.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>037_S_0566</td>\n",
       "      <td>patients_csv/037_S_0566.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>029_S_0843</td>\n",
       "      <td>patients_csv/029_S_0843.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>032_S_1169</td>\n",
       "      <td>patients_csv/032_S_1169.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PTID                         path\n",
       "0    035_S_0204  patients_csv/035_S_0204.pkl\n",
       "1    027_S_0256  patients_csv/027_S_0256.pkl\n",
       "2    114_S_0173  patients_csv/114_S_0173.pkl\n",
       "3    094_S_0921  patients_csv/094_S_0921.pkl\n",
       "4    007_S_1222  patients_csv/007_S_1222.pkl\n",
       "..          ...                          ...\n",
       "377  127_S_1032  patients_csv/127_S_1032.pkl\n",
       "378  057_S_0474  patients_csv/057_S_0474.pkl\n",
       "379  037_S_0566  patients_csv/037_S_0566.pkl\n",
       "380  029_S_0843  patients_csv/029_S_0843.pkl\n",
       "381  032_S_1169  patients_csv/032_S_1169.pkl\n",
       "\n",
       "[382 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = None\n",
    "\n",
    "ad_patient_df = pd.read_csv(\"AD_Patient_Manifest.csv\")\n",
    "\n",
    "ad_patient_df.reset_index(drop=True)\n",
    "\n",
    "ad_patient_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968448f7",
   "metadata": {},
   "source": [
    "#### Split MIMIC-IV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6649bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "# Split MIMIC-IV Dataset into 80-20% for training and testing.\n",
    "train = ad_patient_df.sample(frac=0.8,random_state=200)\n",
    "test = ad_patient_df.drop(train.index)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "# train\n",
    "# train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a952d8",
   "metadata": {},
   "source": [
    "### Define Patient Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35de3f",
   "metadata": {},
   "source": [
    "#### Patient Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502ec60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomDataset gets ADNI cohorts for a NN.\n",
    "class Patient_Dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.manifest = dataframe\n",
    "        self.transform = transform # Apply any given transformations.\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row_entry = self.manifest.iloc[index]['path'] # Get the row (patient) we want to read.\n",
    "        cohort = pd.read_pickle(row_entry)\n",
    "\n",
    "        image = Image.open(cohort.iloc[0]['image_path'])\n",
    "        if self.transform :\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # For Cox Model, label should be in the tuple: duration, event\n",
    "        mmse = torch.tensor(cohort.iloc[0]['MMSE'], dtype=torch.float32)\n",
    "        dx = torch.tensor(cohort.iloc[0]['DX_encoded'], dtype=torch.float32)\n",
    "        label = (mmse, dx)\n",
    "        \n",
    "        demographics = torch.tensor(cohort.iloc[0]['one_hot_vector'], dtype=torch.float32)\n",
    "    \n",
    "        time_series = cohort[['Years_bl', 'ADAS11', 'ADAS13', 'ADASQ4']]\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        time_series_tensor = torch.tensor(time_series.values, dtype=torch.float32)\n",
    "        \n",
    "        patient = (image, demographics, time_series_tensor)\n",
    "        \n",
    "        return patient, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5018763b",
   "metadata": {},
   "source": [
    "#### Define Patient Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e951206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([[[[-1.9980, -2.0323, -2.0323,  ..., -2.1008, -2.0665, -2.0837],\n",
      "          [-2.0665, -2.0837, -2.0665,  ..., -2.0837, -2.0494, -2.0665],\n",
      "          [-2.0323, -2.0494, -2.0494,  ..., -2.0665, -2.0323, -2.0323],\n",
      "          ...,\n",
      "          [-1.9638, -1.9809, -1.9809,  ..., -1.9638, -1.9980, -2.0152],\n",
      "          [-1.9638, -1.9638, -1.9980,  ..., -1.9467, -1.9467, -1.9638],\n",
      "          [-1.9467, -1.9467, -1.9809,  ..., -1.9638, -1.9124, -1.8953]],\n",
      "\n",
      "         [[-1.9132, -1.9482, -1.9482,  ..., -2.0182, -1.9832, -2.0007],\n",
      "          [-1.9832, -2.0007, -1.9832,  ..., -2.0007, -1.9657, -1.9832],\n",
      "          [-1.9482, -1.9657, -1.9657,  ..., -1.9832, -1.9482, -1.9482],\n",
      "          ...,\n",
      "          [-1.8782, -1.8957, -1.8957,  ..., -1.8782, -1.9132, -1.9307],\n",
      "          [-1.8782, -1.8782, -1.9132,  ..., -1.8606, -1.8606, -1.8782],\n",
      "          [-1.8606, -1.8606, -1.8957,  ..., -1.8782, -1.8256, -1.8081]],\n",
      "\n",
      "         [[-1.6824, -1.7173, -1.7173,  ..., -1.7870, -1.7522, -1.7696],\n",
      "          [-1.7522, -1.7696, -1.7522,  ..., -1.7696, -1.7347, -1.7522],\n",
      "          [-1.7173, -1.7347, -1.7347,  ..., -1.7522, -1.7173, -1.7173],\n",
      "          ...,\n",
      "          [-1.6476, -1.6650, -1.6650,  ..., -1.6476, -1.6824, -1.6999],\n",
      "          [-1.6476, -1.6476, -1.6824,  ..., -1.6302, -1.6302, -1.6476],\n",
      "          [-1.6302, -1.6302, -1.6650,  ..., -1.6476, -1.5953, -1.5779]]]]), tensor([[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 0., 0., 0.]]), tensor([[[ 0.0000,  2.3300,  4.3300,  2.0000],\n",
      "         [ 0.5339,  4.6700,  9.6700,  4.0000],\n",
      "         [ 1.0322,  5.3300,  7.3300,  2.0000],\n",
      "         [ 2.0287,  5.0000,  7.0000,  2.0000],\n",
      "         [ 2.9733,  6.0000, 10.0000,  4.0000]]])], [tensor([30.]), tensor([0.])]]\n",
      "[[tensor([[[[-0.3198, -0.3369, -0.3541,  ..., -1.5014, -1.2445, -1.0733],\n",
      "          [-0.2684, -0.2856, -0.3198,  ..., -1.5528, -1.2959, -1.1247],\n",
      "          [-0.1828, -0.2171, -0.2684,  ..., -1.6042, -1.3644, -1.2103],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-0.1975, -0.2150, -0.2325,  ..., -1.4055, -1.1429, -0.9678],\n",
      "          [-0.1450, -0.1625, -0.1975,  ..., -1.4580, -1.1954, -1.0203],\n",
      "          [-0.0574, -0.0924, -0.1450,  ..., -1.5105, -1.2654, -1.1078],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[ 0.0256,  0.0082, -0.0092,  ..., -1.1770, -0.9156, -0.7413],\n",
      "          [ 0.0779,  0.0605,  0.0256,  ..., -1.2293, -0.9678, -0.7936],\n",
      "          [ 0.1651,  0.1302,  0.0779,  ..., -1.2816, -1.0376, -0.8807],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]), tensor([[0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 0., 0., 0.]]), tensor([[[ 0.0000, 10.6700, 19.6700,  8.0000],\n",
      "         [ 0.4928, 10.0000, 19.0000,  8.0000],\n",
      "         [ 0.9966, 11.3300, 18.3300,  6.0000],\n",
      "         [ 2.1656, 11.3300, 20.3300,  7.0000],\n",
      "         [ 3.0089, 13.0000, 21.0000,  6.0000]]])], [tensor([28.]), tensor([0.])]]\n",
      "[[tensor([[[[-2.0494, -2.0837, -2.1008,  ..., -2.0323, -2.0323, -2.0152],\n",
      "          [-2.0665, -2.0665, -2.0665,  ..., -2.0152, -2.0152, -1.9980],\n",
      "          [-2.0837, -2.0665, -2.0494,  ..., -1.9980, -1.9980, -1.9638],\n",
      "          ...,\n",
      "          [-2.0837, -2.0494, -2.0494,  ..., -2.0665, -2.0665, -2.0665],\n",
      "          [-2.0837, -2.0665, -2.0665,  ..., -2.0323, -2.0323, -2.0152],\n",
      "          [-2.0665, -2.0665, -2.0665,  ..., -2.0152, -1.9980, -1.9638]],\n",
      "\n",
      "         [[-1.9657, -2.0007, -2.0182,  ..., -1.9482, -1.9482, -1.9307],\n",
      "          [-1.9832, -1.9832, -1.9832,  ..., -1.9307, -1.9307, -1.9132],\n",
      "          [-2.0007, -1.9832, -1.9657,  ..., -1.9132, -1.9132, -1.8782],\n",
      "          ...,\n",
      "          [-2.0007, -1.9657, -1.9657,  ..., -1.9832, -1.9832, -1.9832],\n",
      "          [-2.0007, -1.9832, -1.9832,  ..., -1.9482, -1.9482, -1.9307],\n",
      "          [-1.9832, -1.9832, -1.9832,  ..., -1.9307, -1.9132, -1.8782]],\n",
      "\n",
      "         [[-1.7347, -1.7696, -1.7870,  ..., -1.7173, -1.7173, -1.6999],\n",
      "          [-1.7522, -1.7522, -1.7522,  ..., -1.6999, -1.6999, -1.6824],\n",
      "          [-1.7696, -1.7522, -1.7347,  ..., -1.6824, -1.6824, -1.6476],\n",
      "          ...,\n",
      "          [-1.7696, -1.7347, -1.7347,  ..., -1.7522, -1.7522, -1.7522],\n",
      "          [-1.7696, -1.7522, -1.7522,  ..., -1.7173, -1.7173, -1.6999],\n",
      "          [-1.7522, -1.7522, -1.7522,  ..., -1.6999, -1.6824, -1.6476]]]]), tensor([[1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 0., 0., 0.]]), tensor([[[ 0.0000, 34.3300, 49.3300, 10.0000],\n",
      "         [ 0.5202, 22.0000, 51.5533, 10.0000],\n",
      "         [ 0.9774, 34.0000, 49.0000, 10.0000],\n",
      "         [ 2.1246, 41.3300, 56.3300, 10.0000]]])], [tensor([25.]), tensor([1.])]]\n",
      "[[tensor([[[[-1.3473, -1.2274, -1.0562,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-1.2445, -1.2274, -1.2274,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-1.2617, -1.3302, -1.4329,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-1.2479, -1.1253, -0.9503,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-1.1429, -1.1253, -1.1253,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-1.1604, -1.2304, -1.3354,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.0201, -0.8981, -0.7238,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-0.9156, -0.8981, -0.8981,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-0.9330, -1.0027, -1.1073,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]), tensor([[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 0., 0., 0.]]), tensor([[[ 0.0000, 15.0000, 24.0000,  8.0000],\n",
      "         [ 0.6516, 15.6700, 24.6700,  8.0000],\n",
      "         [ 1.1170, 15.6700, 24.6700,  8.0000],\n",
      "         [ 1.4949, 17.3300, 25.3300,  8.0000],\n",
      "         [ 2.0287, 12.6700, 21.6700,  8.0000],\n",
      "         [ 3.0883, 13.0000, 21.0000,  7.0000]]])], [tensor([27.]), tensor([2.])]]\n",
      "[[tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-0.0972, -0.0801,  0.0912,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [ 0.1939, -0.0116, -0.1486,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [ 0.3309,  0.1254, -0.1143,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [ 0.0301,  0.0476,  0.2227,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [ 0.3277,  0.1176, -0.0224,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [ 0.4678,  0.2577,  0.0126,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [ 0.2522,  0.2696,  0.4439,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [ 0.5485,  0.3393,  0.1999,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [ 0.6879,  0.4788,  0.2348,  ..., -1.8044, -1.8044, -1.8044]]]]), tensor([[0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "         0., 0., 0., 1., 0., 1., 0., 0., 0.]]), tensor([[[ 0.0000,  5.0000,  9.0000,  4.0000],\n",
      "         [ 0.5120,  9.6700, 13.6700,  4.0000],\n",
      "         [ 0.9829,  7.0000, 11.0000,  4.0000],\n",
      "         [ 1.9986,  6.0000, 10.0000,  4.0000],\n",
      "         [ 3.0034,  3.3300,  4.3300,  1.0000]]])], [tensor([29.]), tensor([0.])]]\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "# Will need to be applied by passing in to Dataset constructor!\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5), (0.5))\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5), (0.5))\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "dl_args = dict(batch_size=16, num_workers=4)\n",
    "\n",
    "# Dataloaders used to iterate through the patients. Patients split 80-20% into train-test loaders.\n",
    "train_dataset = Patient_Dataset(train, data_transforms['train'])\n",
    "train_dataset.transform = data_transforms['train']\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "\n",
    "test_dataset = Patient_Dataset(test, data_transforms['test'])\n",
    "test_dataset.transform = data_transforms['test']\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "dataloaders = {'train': train_dataloader,\n",
    "              'test': test_dataloader,\n",
    "              }\n",
    "\n",
    "i = 0\n",
    "for patient in dataloaders['train']:\n",
    "    i = i + 1\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(patient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9f548d",
   "metadata": {},
   "source": [
    "### Time Series Dataset & Dataloader\n",
    "!!! Used for training autoencoder only !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1878259",
   "metadata": {},
   "source": [
    "#### Custom Collate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab5b6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn handles padding on inputs.\n",
    "def collate_fn(batch):\n",
    "    # Separate the data and labels\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Pad sequences\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    # Stack labels into a tensor\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return padded_sequences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47186ccc",
   "metadata": {},
   "source": [
    "#### Time Series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc53004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize encoders\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "class InHospitalMortalityDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.manifest = dataframe\n",
    "        self.transform = transform # Apply any given transformations.\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.manifest)    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row_entry = self.manifest.iloc[index]['path'] # Get the row (patient) we want to read.\n",
    "        cohort = pd.read_pickle(row_entry)\n",
    "        \n",
    "        time_series_data = cohort[['Years_bl', 'ADAS11', 'ADAS13', 'ADASQ4']]\n",
    "        \n",
    "        time_series_tensor = torch.tensor(time_series_data.values, dtype=torch.float32)\n",
    "        \n",
    "        return time_series_tensor, time_series_tensor  # Input and target are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0a8775",
   "metadata": {},
   "source": [
    "#### Time Series Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0ac601a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000, 13.0000, 22.0000,  8.0000],\n",
      "         [ 0.5886, 15.3300, 27.3300, 10.0000],\n",
      "         [ 1.0212,  9.6700, 20.6700,  9.0000],\n",
      "         [ 1.5003, 18.0000, 28.0000, 10.0000],\n",
      "         [ 2.0370, 15.3300, 26.3300, 10.0000],\n",
      "         [ 3.0089, 16.3300, 27.3300, 10.0000]]])\n",
      "tensor([[[ 0.0000,  6.0000,  9.0000,  3.0000],\n",
      "         [ 0.5175,  5.3300, 10.3300,  4.0000],\n",
      "         [ 1.0240,  4.6700,  7.6700,  3.0000],\n",
      "         [ 2.0589,  1.0000,  2.0000,  1.0000],\n",
      "         [ 3.2115,  3.0000,  7.0000,  2.0000]]])\n",
      "tensor([[[ 0.0000,  4.0000, 11.0000,  6.0000],\n",
      "         [ 0.4956,  7.3300, 18.3300,  9.0000],\n",
      "         [ 0.9938, 12.0000, 20.0000,  7.0000],\n",
      "         [ 1.4949, 10.6700, 18.6700,  7.0000],\n",
      "         [ 1.9932, 13.6700, 22.6700,  7.0000],\n",
      "         [ 3.0664, 21.6700, 36.6700, 10.0000]]])\n",
      "tensor([[[ 0.0000, 10.3300, 15.3300,  5.0000],\n",
      "         [ 0.4791,  5.3300,  7.3300,  2.0000],\n",
      "         [ 0.9747,  5.6700,  8.6700,  3.0000],\n",
      "         [ 2.0068,  4.0000,  6.0000,  2.0000],\n",
      "         [ 3.0226,  4.3300,  8.3300,  4.0000]]])\n",
      "tensor([[[ 0.0000,  6.6700,  9.6700,  3.0000],\n",
      "         [ 0.5038, 10.0000, 14.0000,  4.0000],\n",
      "         [ 0.9993,  4.0000,  9.0000,  5.0000],\n",
      "         [ 1.5168, 17.3300, 20.3300,  3.0000],\n",
      "         [ 1.9576,  8.6700, 13.6700,  5.0000],\n",
      "         [ 3.0034,  8.6700, 16.6700,  6.0000]]])\n"
     ]
    }
   ],
   "source": [
    "train_listfile_path = 'time_series_list.csv'\n",
    "train_files_path = '/home/mason/TFN/patient_time_series'\n",
    "batch_size = 1\n",
    "\n",
    "time_train_dataset = InHospitalMortalityDataset(ad_patient_df, None)\n",
    "\n",
    "time_train_loader = DataLoader(dataset=time_train_dataset, batch_size=batch_size, shuffle=True, \\\n",
    "                               collate_fn=collate_fn)\n",
    "\n",
    "i = 0\n",
    "for time_series, label in time_train_loader:\n",
    "    i = i + 1\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd94733",
   "metadata": {},
   "source": [
    "# Define Models\n",
    "\n",
    "The Fusion Network will fuse the embeddings of three input models---X-Ray, Demographics, and Time Series Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4f3966",
   "metadata": {},
   "source": [
    "### X-Ray Embedder\n",
    "\n",
    "(VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3818ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedVGG16Model(torch.nn.Module):\n",
    "    def __init__(self, model=None):\n",
    "        super(ModifiedVGG16Model, self).__init__()\n",
    "\n",
    "        model = models.vgg16(weights='IMAGENET1K_V1')\n",
    "        self.features = model.features\n",
    "        \n",
    "        # When embedding, we only want the output of the first FC layer.\n",
    "        self.embedder = nn.Sequential(\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True)) # This last ReLU layer may be unnecessary\n",
    "        \n",
    "        # Contains the Tail of VGG16 (all 3 FC layers and ReLU, when combined with embedder)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4096, 2),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.embedder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def embed(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.embedder(x)\n",
    "        # May need nn.flatten()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442437c",
   "metadata": {},
   "source": [
    "### Demographics Embedder\n",
    "\n",
    "Ingests:\n",
    " - Age\n",
    " - Gender\n",
    " - Martial Status\n",
    " - Ethnicity\n",
    "\n",
    "All words will need to be reshaped to a common length (set by the longest word/string) and will be concatonated from there so the final dimension of the input tensor (representing concat words) is understood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d58cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemographicsEmbedder(torch.nn.Module):\n",
    "    def __init__(self, model=None):\n",
    "        super(DemographicsEmbedder, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool1d(2))\n",
    "        \n",
    "        self.embedder = nn.Sequential(\n",
    "            # nn.Dropout(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(208, 70),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(70, 10),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(10, 1))\n",
    "        \n",
    "    def forward(self, demographics):\n",
    "        x = demographics\n",
    "        x = x.view(1, 1, 27) # Reshape tensor to [N, C, L] format expected by Conv1D\n",
    "        x = self.features(x)\n",
    "        x = self.embedder(x)\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "    \n",
    "    def embed(self, demographics):\n",
    "        x = demographics\n",
    "        x = x.view(1, 1, 27) # Reshape tensor to [N, C, L] format expected by Conv1D\n",
    "        x = self.features(x)\n",
    "        x = self.embedder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c3a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0894]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0957]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0936]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0923]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0961]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test Demographics Embedder\n",
    "def test_demographics():\n",
    "    test_demo_embedder = DemographicsEmbedder()\n",
    "\n",
    "    i = 0\n",
    "    for entry in dataloaders['train']:\n",
    "        patient = entry[0]\n",
    "        label = entry[1]\n",
    "        \n",
    "        demographics = patient[1]\n",
    "        \n",
    "        i = i + 1\n",
    "        if i > 5:\n",
    "            break\n",
    "    #     print(patient)\n",
    "    #     print(patient[2])\n",
    "    #     print(patient[2][1][0])\n",
    "    #     print(type(patient[2][1][0]))\n",
    "        print(test_demo_embedder.forward(demographics))\n",
    "        \n",
    "test_demographics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b5f9d",
   "metadata": {},
   "source": [
    "### Time Series Data Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d5079",
   "metadata": {},
   "source": [
    "#### Define Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "903b8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_Series_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(Time_Series_Autoencoder, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.hidden_to_latent = nn.Linear(hidden_size, latent_size)\n",
    "        self.latent_to_hidden = nn.Linear(latent_size, hidden_size)\n",
    "        self.decoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "#         self.decoder = nn.LSTM(hidden_size, input_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)  # Additional final linear layer\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequence\n",
    "        packed_x = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Encoder\n",
    "        packed_h, (h, c) = self.encoder(packed_x)\n",
    "        h = h[-1]  # Get the hidden state from the last layer of the LSTM\n",
    "        latent = self.hidden_to_latent(h)\n",
    "        \n",
    "        # Prepare for decoder\n",
    "        hidden = self.latent_to_hidden(latent).unsqueeze(0)\n",
    "        cell = torch.zeros_like(hidden)\n",
    "        \n",
    "        # Decoder\n",
    "        packed_output, _ = self.decoder(packed_x, (hidden, cell))\n",
    "        \n",
    "        # Pad the packed sequence\n",
    "        decoded, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Apply the final linear layer to map hidden state to input size\n",
    "        decoded = self.output_layer(decoded)\n",
    "        \n",
    "        return latent, decoded\n",
    "    \n",
    "    def encode(self, x, lengths):\n",
    "        with torch.no_grad():\n",
    "            # Pack the padded sequence\n",
    "            packed_x = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "            \n",
    "            # Encoder\n",
    "            packed_h, (h, c) = self.encoder(packed_x)\n",
    "            h = h[-1]  # Get the hidden state from the last layer of the LSTM\n",
    "            latent = self.hidden_to_latent(h)\n",
    "        \n",
    "        return latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc400149",
   "metadata": {},
   "source": [
    "#### Autoencoder without Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3742f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape torch.Size([1, 4, 4])\n",
      "Latent representation shape: torch.Size([1, 8])\n",
      "Reconstructed shape: torch.Size([1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Define Autoencoder Parameters\n",
    "input_size = AUTOENCODER_INPUTS  # Number of features in time series data\n",
    "hidden_size = 32\n",
    "latent_size = 8 # Attempt to get good bottleneck, given large length of time-series data.\n",
    "\n",
    "test_autoencoder = Time_Series_Autoencoder(input_size, hidden_size, latent_size)\n",
    "\n",
    "# test_autoencoder.to(device)\n",
    "\n",
    "# Forward pass example\n",
    "for time_series, label in time_train_loader:\n",
    "    lengths = [len(seq) for seq in time_series]\n",
    "    latent, reconstructed = test_autoencoder(time_series, lengths)\n",
    "    print(\"Original Shape\", time_series.shape)\n",
    "    print(\"Latent representation shape:\", latent.shape)\n",
    "    print(\"Reconstructed shape:\", reconstructed.shape)\n",
    "#     print(time_series)\n",
    "#     print(latent)\n",
    "#     print(reconstructed)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29736bf9",
   "metadata": {},
   "source": [
    "#### Training Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7d564eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, dataloader, num_epochs, learning_rate):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    print(\"Beginning Training\")\n",
    "    print(\"Total Entries:\", len(dataloader))\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        currIdx = 0\n",
    "        for time_series, label in dataloader:\n",
    "            # Calculate lengths of sequences (non-zero length for padded sequences)\n",
    "#             print(time_series)\n",
    "            lengths = [len(seq) for seq in time_series]\n",
    "#             print(\"Lengths Calculated\")\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            latent, reconstructed = model(time_series, lengths)\n",
    "\n",
    "            loss = criterion(reconstructed, time_series)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "#             print(\"Original Sequence:\")\n",
    "#             print(sequences)\n",
    "#             print(\"Autoencoder Reconstruction:\")\n",
    "#             print(reconstructed)\n",
    "#             print(\"Loss:\")\n",
    "#             print(loss)\n",
    "        \n",
    "            currIdx = currIdx + 1\n",
    "            if (currIdx % 100 == 0):\n",
    "                print(\"Current Entry:\", currIdx)\n",
    "                print(\"Current Loss:\", loss.item())\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "        \n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cdf4464",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 6\n",
    "learning_rate = 0.001\n",
    "\n",
    "# train_autoencoder(test_autoencoder, time_train_loader, num_epochs, learning_rate)\n",
    "\n",
    "# torch.save(test_autoencoder, \"TFN_AEC_R1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68a7da",
   "metadata": {},
   "source": [
    "#### Autoencoder after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1994ad0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape torch.Size([1, 5, 4])\n",
      "Latent representation shape: torch.Size([1, 8])\n",
      "Reconstructed shape: torch.Size([1, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "completed_autoencoder = torch.load('TFN_AEC_R1')#.to(device)\n",
    "\n",
    "# completed_autoencoder.to(device)\n",
    "\n",
    "for time_series, label in time_train_loader:\n",
    "    lengths = [len(seq) for seq in time_series]\n",
    "    latent, reconstructed = completed_autoencoder(time_series, lengths)\n",
    "    print(\"Original Shape\", time_series.shape)\n",
    "    print(\"Latent representation shape:\", latent.shape)\n",
    "    print(\"Reconstructed shape:\", reconstructed.shape)\n",
    "#     print(time_series)\n",
    "#     print(latent)\n",
    "#     print(reconstructed)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47df8ea",
   "metadata": {},
   "source": [
    "# Medical Fusion Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3131b26",
   "metadata": {},
   "source": [
    "### Complete Model - Fusion\n",
    "\n",
    "(Contains X-Ray Embedder and Demographics Embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c8a9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalFusionNetwork(torch.nn.Module):\n",
    "    def __init__(self, model=None):\n",
    "        super(MedicalFusionNetwork, self).__init__()\n",
    "        \n",
    "        self.visual_embedder = ModifiedVGG16Model()\n",
    "        self.demographics_embedder = DemographicsEmbedder()\n",
    "        self.autoencoder = torch.load('TFN_AEC_R1')\n",
    "        \n",
    "        self.regression = nn.Sequential(\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(360, 360),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(360, 1))\n",
    "        \n",
    "        self.classification = nn.Sequential(\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(360, 360),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(360, 3))\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def tensor_fusion(self, imagery, demographics, time_series):\n",
    "        # Get feature outputs from model subnets.\n",
    "        visual_features = self.visual_embedder.embed(imagery)\n",
    "        demographic_features = self.demographics_embedder.embed(demographics)\n",
    "        lengths = [len(seq) for seq in time_series]\n",
    "        latent, reconstructed = self.autoencoder(time_series, lengths)\n",
    "\n",
    "        # Concat 1's onto feature vectors to prepare for Tensor Fusion\n",
    "        visual_h = torch.concat((torch.ones(1, 1).to(device), visual_features), dim=1)\n",
    "        demographics_h = torch.concat((torch.ones(1, 1).to(device), demographic_features), dim=1)\n",
    "        time_series_h = torch.concat((torch.ones(1, 1).to(device), latent), dim=1)\n",
    "    \n",
    "        # Perform Tensor Fusion (Kronecker Product)\n",
    "        outer_xy = torch.einsum('bi,bj->bij', demographics_h, time_series_h)  # Shape: (batch_size, x_dim, y_dim)\n",
    "        outer_xyz = torch.einsum('bij,bk->bijk', outer_xy, visual_h)  # Shape: (batch_size, x_dim, y_dim, z_dim)\n",
    "\n",
    "        # Prevent tensor values from becoming extreme.\n",
    "        outer_xyz = torch.clamp(outer_xyz, min=-1e6, max=1e6)\n",
    "        \n",
    "        return outer_xyz\n",
    "        \n",
    "    def tucker_feature_extraction(self, patient):\n",
    "        imagery = patient[0]\n",
    "        demographics = patient[1]\n",
    "        time_series = patient[2]\n",
    "        \n",
    "        # Tensor Fusion\n",
    "        outer_xyz = self.tensor_fusion(imagery, demographics, time_series).to('cpu')\n",
    "        \n",
    "        # Tucker Decomposition      \n",
    "        with torch.no_grad():\n",
    "            (core, factors), rec_error = partial_tucker(outer_xyz, modes=[0, 1, 2, 3], rank=[1, 6, 6, 10])\n",
    "            # README: Make table comparing rank output performance\n",
    "        \n",
    "        # Flatten the resulting tensor for use in FC layer\n",
    "        outer_xyz_flattened = core.reshape(core.size(0), -1)\n",
    "        \n",
    "        # print(\"Decomposed Size:\", outer_xyz_flattened.size())\n",
    "        \n",
    "        return outer_xyz_flattened\n",
    "        \n",
    "    def regression_classification(self, patient):\n",
    "        fusion = self.tucker_feature_extraction(patient).to(device)\n",
    "        \n",
    "        los = self.regression(fusion)\n",
    "        mortality = self.classification(fusion)\n",
    "#         mortality = self.softmax(mortality)\n",
    "        out = (los, mortality)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, patient):\n",
    "        fusion = self.tucker_feature_extraction(patient)\n",
    "    \n",
    "        return fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a2cde",
   "metadata": {},
   "source": [
    "## Testing Complete Model\n",
    "\n",
    "Untrained Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "939c25a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:\n",
      "(tensor([30.], device='cuda:1'), tensor([0.], device='cuda:1'))\n",
      "Prediction:\n",
      "(tensor([[-0.4502]], device='cuda:1', grad_fn=<AddmmBackward0>), tensor([[-0.5402,  0.5060,  0.1398]], device='cuda:1',\n",
      "       grad_fn=<AddmmBackward0>))\n",
      "Label:\n",
      "(tensor([28.], device='cuda:1'), tensor([0.], device='cuda:1'))\n",
      "Prediction:\n",
      "(tensor([[-0.5432]], device='cuda:1', grad_fn=<AddmmBackward0>), tensor([[-0.6639,  0.6021,  0.1645]], device='cuda:1',\n",
      "       grad_fn=<AddmmBackward0>))\n",
      "Label:\n",
      "(tensor([25.], device='cuda:1'), tensor([1.], device='cuda:1'))\n",
      "Prediction:\n",
      "(tensor([[-0.4610]], device='cuda:1', grad_fn=<AddmmBackward0>), tensor([[-0.5545,  0.5172,  0.1427]], device='cuda:1',\n",
      "       grad_fn=<AddmmBackward0>))\n",
      "Label:\n",
      "(tensor([27.], device='cuda:1'), tensor([2.], device='cuda:1'))\n",
      "Prediction:\n",
      "(tensor([[-0.4741]], device='cuda:1', grad_fn=<AddmmBackward0>), tensor([[-0.5720,  0.5308,  0.1462]], device='cuda:1',\n",
      "       grad_fn=<AddmmBackward0>))\n",
      "Label:\n",
      "(tensor([29.], device='cuda:1'), tensor([0.], device='cuda:1'))\n",
      "Prediction:\n",
      "(tensor([[-0.4326]], device='cuda:1', grad_fn=<AddmmBackward0>), tensor([[-0.5167,  0.4878,  0.1351]], device='cuda:1',\n",
      "       grad_fn=<AddmmBackward0>))\n"
     ]
    }
   ],
   "source": [
    "MFN = MedicalFusionNetwork()\n",
    "MFN.to(device)\n",
    "\n",
    "def test_medical_net():\n",
    "    i = 0\n",
    "    for entry in dataloaders['train']:\n",
    "        i = i + 1\n",
    "        if i > 5:\n",
    "            break\n",
    "        patient = entry[0]\n",
    "        patient = (patient[0].to(device), patient[1].to(device), patient[2].to(device))\n",
    "        label = entry[1]\n",
    "        label = (label[0].to(device), label[1].to(device))\n",
    "        pred = MFN.regression_classification(patient)\n",
    "        print(\"Label:\")\n",
    "        print(label)\n",
    "        print(\"Prediction:\")\n",
    "        print(pred)\n",
    "        \n",
    "test_medical_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66421f0a",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c88de",
   "metadata": {},
   "source": [
    "### Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e00c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, dataloaders, model, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.classification_criterion = nn.CrossEntropyLoss()\n",
    "        self.regression_criterion = nn.MSELoss()\n",
    "        self.model.train()\n",
    "        self.train_regression_loss = []\n",
    "        self.train_classification_loss = []\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        total = 0\n",
    "        total_time = 0\n",
    "        running_regression_loss = 0.0\n",
    "        running_classification_loss = 0.0\n",
    "        classification_predictions = []\n",
    "        classification_labels = []\n",
    "        \n",
    "        for i, (entry) in enumerate(dataloaders['test']):\n",
    "            # Get image, demographics, and label from dataloader and send to device.\n",
    "            patient = entry[0]\n",
    "            patient = (patient[0].to(device), patient[1].to(device), patient[2].to(device))\n",
    "            label = entry[1]\n",
    "            label = (label[0].to(device), label[1].to(device))\n",
    "            regression_label = label[0].to(device).unsqueeze(1)\n",
    "            classification_label = label[1].to(device)\n",
    "            classification_label = classification_label.long()\n",
    "            \n",
    "            # Start keeping time, and run model for output.\n",
    "            t0 = time.time()\n",
    "            \n",
    "            output = self.model.regression_classification(patient)\n",
    "            \n",
    "            t1 = time.time()\n",
    "            total_time = total_time + (t1 - t0)\n",
    "            \n",
    "            # Calculate item loss\n",
    "            model_regression = output[0]\n",
    "            model_classification = output[1].float()\n",
    "            \n",
    "#             print(classification_label)\n",
    "#             print(F.softmax(model_classification, dim=1))\n",
    "            \n",
    "            # Keep track of current classification target\n",
    "            classification_labels.extend(classification_label.float().to(\"cpu\"))\n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = F.softmax(model_classification, dim=1)\n",
    "            # Use argmax to get the index of the class with the highest probability\n",
    "            predicted_class = torch.argmax(probabilities, dim=1)\n",
    "            # extend output tracker \n",
    "            classification_predictions.extend(predicted_class.to(\"cpu\").detach().numpy().tolist())\n",
    "            \n",
    "            regression_loss = self.regression_criterion(model_regression, regression_label)\n",
    "            classification_loss = self.classification_criterion(model_classification, classification_label)\n",
    "            \n",
    "            # Add to running total\n",
    "            running_regression_loss += regression_loss.item()\n",
    "            running_classification_loss += classification_loss.item()\n",
    "        \n",
    "        # Get average loss\n",
    "        total_regression_loss = running_regression_loss / len(dataloaders['test'])\n",
    "        total_classification_loss = running_classification_loss / len(dataloaders['test'])\n",
    "        \n",
    "        # Print model training time and statistics.\n",
    "        print(\"=== Regression Accuracy ===\")\n",
    "        print(\"Mean Squared Error:\", total_regression_loss)\n",
    "        print(\"=== Classification Accuracy ===\")\n",
    "        classification_labels = [ int(x.item()) for x in classification_labels ]\n",
    "        classification_predictions = [ round(elem) for elem in classification_predictions ]\n",
    "        print(\"Accuracy Score:\", accuracy_score(classification_labels, classification_predictions))\n",
    "        print(\"Cross Entropy Loss:\", total_classification_loss)\n",
    "        calc_time = float(total_time) / (i + 1)\n",
    "        print(\"Total Prediction Time:\", total_time)\n",
    "        print('Average Prediction Time: {min}m {sec}s'.format(min=calc_time // 60.0, sec=calc_time % 60.0))\n",
    "        print(\"Total Entries Compared: \", i + 1)\n",
    "#         print(outputs)\n",
    "        \n",
    "        return (total_time, classification_labels, classification_predictions)\n",
    "\n",
    "    def train(self, epoches=10):\n",
    "        since = time.time()\n",
    "        self.train_regression_loss = []\n",
    "        self.train_classification_loss = []\n",
    "        \n",
    "        for i in range(epoches):\n",
    "            print(\"Epoch: \", i)\n",
    "            self.train_epoch()\n",
    "            self.test()\n",
    "            self.model.eval()\n",
    "            \n",
    "        print(\"Finished fine tuning.\")\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in  {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        \n",
    "\n",
    "    def train_batch(self, patient, regression_label, classification_label):\n",
    "        self.model.train()\n",
    "        # Not sure why double works fine for forward, but not for backwards. .float() is here to fix this.\n",
    "\n",
    "        output = self.model.regression_classification(patient)\n",
    "        model_regression = output[0]\n",
    "        model_classification = output[1].float()\n",
    "#         model_classification = model_classification.squeeze()\n",
    "        classification_label = classification_label.squeeze(dim=0).long()\n",
    "#         print(model_classification.shape)\n",
    "#         print(model_classification)\n",
    "#         print(classification_label.shape)\n",
    "#         print(classification_label)\n",
    "        \n",
    "        regression_loss = self.regression_criterion(model_regression, regression_label)\n",
    "        \n",
    "        classification_loss = self.classification_criterion(model_classification, classification_label)\n",
    "        \n",
    "        # main_weight = 0.0\n",
    "        # aux_weight = 2.0\n",
    "        main_weight = 0.5\n",
    "        aux_weight = 0.5\n",
    "        \n",
    "        total_loss = (main_weight * regression_loss) + (aux_weight * classification_loss)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(MFN.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        for i, (entry) in enumerate(dataloaders['train']):\n",
    "            # NOTE: Disable model.to(device) for better traceback.\n",
    "#             self.model.to(device)\n",
    "            patient = entry[0]\n",
    "            patient = (patient[0].to(device), patient[1].to(device), patient[2].to(device))\n",
    "            label = entry[1]\n",
    "            label = (label[0].to(device), label[1].to(device))\n",
    "            regression_label = label[0].to(device)\n",
    "            classification_label = label[1].to(device)\n",
    "            # Convert input from [1] to [1, 1] size to match input.\n",
    "            regression_label = regression_label.unsqueeze(1)\n",
    "            classification_label = classification_label.unsqueeze(1)\n",
    "            self.train_batch(patient, regression_label, classification_label)\n",
    "            if(i % 1000 == 0):\n",
    "                print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6be884ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "130",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mason/.conda/envs/LOSresearch/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "sys.exit(130)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f5210",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21346568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 117.86605568935997\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.23684210526315788\n",
      "Cross Entropy Loss: 1.11537861039764\n",
      "Total Prediction Time: 4.377004861831665\n",
      "Average Prediction Time: 0.0m 0.05759216923462717s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  1\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 17.15449435636401\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.0939374793516963\n",
      "Total Prediction Time: 4.424644708633423\n",
      "Average Prediction Time: 0.0m 0.05821900932412399s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  2\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 17.32528924215206\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.1506556209764982\n",
      "Total Prediction Time: 4.367140769958496\n",
      "Average Prediction Time: 0.0m 0.057462378552085476s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  3\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 17.99311810435335\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.1074447937701877\n",
      "Total Prediction Time: 4.392070055007935\n",
      "Average Prediction Time: 0.0m 0.057790395460630715s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  4\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 15.684286445947548\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.0782950551886308\n",
      "Total Prediction Time: 4.390426158905029\n",
      "Average Prediction Time: 0.0m 0.05776876524875039s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  5\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 16.88947640327847\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.1032921397372295\n",
      "Total Prediction Time: 4.433435440063477\n",
      "Average Prediction Time: 0.0m 0.05833467684294048s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  6\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 15.893957191002922\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.0854675691378743\n",
      "Total Prediction Time: 4.400882005691528\n",
      "Average Prediction Time: 0.0m 0.05790634218015169s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  7\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 16.72781893694666\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.087960161660847\n",
      "Total Prediction Time: 4.410263776779175\n",
      "Average Prediction Time: 0.0m 0.05802978653656809s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  8\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 15.142213613180886\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.087762906363136\n",
      "Total Prediction Time: 4.565818786621094\n",
      "Average Prediction Time: 0.0m 0.0600765629818565s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  9\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 15.38124566657873\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.0818566954449604\n",
      "Total Prediction Time: 4.669901371002197\n",
      "Average Prediction Time: 0.0m 0.06144607067108154s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  10\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 16.61209397027759\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.0800203728048425\n",
      "Total Prediction Time: 4.65462589263916\n",
      "Average Prediction Time: 0.0m 0.06124507753472579s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  11\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 17.416379807500082\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.0821591560777866\n",
      "Total Prediction Time: 4.582518577575684\n",
      "Average Prediction Time: 0.0m 0.06029629707336426s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  12\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 14.530411093544803\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.085506772524432\n",
      "Total Prediction Time: 4.7726616859436035\n",
      "Average Prediction Time: 0.0m 0.06279818007820531s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  13\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 14.997386825883662\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.081226301036383\n",
      "Total Prediction Time: 4.778478145599365\n",
      "Average Prediction Time: 0.0m 0.06287471244209691s\n",
      "Total Entries Compared:  76\n",
      "Epoch:  14\n",
      "0\n",
      "=== Regression Accuracy ===\n",
      "Mean Squared Error: 17.840872474112793\n",
      "=== Classification Accuracy ===\n",
      "Accuracy Score: 0.3815789473684211\n",
      "Cross Entropy Loss: 1.0976654777401371\n",
      "Total Prediction Time: 4.794423341751099\n",
      "Average Prediction Time: 0.0m 0.06308451765461971s\n",
      "Total Entries Compared:  76\n",
      "Finished fine tuning.\n",
      "Training complete in  6m 26s\n"
     ]
    }
   ],
   "source": [
    "# MFN = torch.load(\"AD_TFN_R3\")\n",
    "\n",
    "# optimizer = torch.optim.Adam(MFN.parameters(), lr=0.00007)\n",
    "optimizer = torch.optim.Adam(MFN.parameters(), lr=0.0001)\n",
    "trainer = Trainer(datasets, MFN, optimizer)\n",
    "\n",
    "# _ = trainer.test()\n",
    "\n",
    "trainer.train(15)\n",
    "\n",
    "torch.save(MFN, \"AD_TFN_R4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f48fa",
   "metadata": {},
   "source": [
    "=== Regression Accuracy ===\n",
    "Mean Squared Error: 7.7321083134624455\n",
    "=== Classification Accuracy ===\n",
    "Cross Binary Entropy Loss: 1.0958607646128151\n",
    "Total Prediction Time: 4.07979154586792\n",
    "Average Prediction Time: 0.0m 0.025029395986919754s\n",
    "Total Entries Compared:  163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, labels, preds = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a808b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903cf26d",
   "metadata": {},
   "source": [
    "#### Model after Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_concat = torch.load('6aec_concat_mfn_R2')\n",
    "\n",
    "trained_concat_trainer = Trainer(datasets, trained_concat, optimizer)\n",
    "\n",
    "_, _, _, _, _ = trained_concat_trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_fusion = torch.load(\"6aec_fusion_mfn\")\n",
    "\n",
    "i = 0\n",
    "for entry in dataloaders['test']:\n",
    "    i = i + 1\n",
    "    if i > 5:\n",
    "        break\n",
    "    patient = entry[0]\n",
    "    patient = (patient[0].to(device), patient[1].to(device), patient[2].to(device))\n",
    "    label = entry[1]\n",
    "    label = (label[0].to(device), label[1].to(device))\n",
    "    pred = trained_fusion.regression_classification(patient)\n",
    "    print(\"Label:\")\n",
    "    print(label)\n",
    "    print(\"Prediction:\")\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa3a9d",
   "metadata": {},
   "source": [
    "# Perform Decomposition on completed TFN Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28dc90a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine whether to use CPD/PARAFAC. Uses Tucker if False.\n",
    "cp = False\n",
    "decompose = False\n",
    "origin_model = '6aec_concat_mfn_R2'\n",
    "save_name = '6aec_concat_mfn_R2_decomposed'\n",
    "\n",
    "if decompose:\n",
    "    model = torch.load(origin_model).to(device)\n",
    "    model.eval()\n",
    "    model.to(\"cpu\") # FIXME: Original code moves model to GPU, the CPU. Unnecessary?\n",
    "    N = len(model.visual_embedder.features._modules.keys())\n",
    "    for i, key in enumerate(model.visual_embedder.features._modules.keys()):\n",
    "\n",
    "        if i >= N - 2:\n",
    "            break\n",
    "        if isinstance(model.visual_embedder.features._modules[key], torch.nn.modules.conv.Conv2d):\n",
    "            conv_layer = model.visual_embedder.features._modules[key]\n",
    "            if cp:\n",
    "                rank = max(conv_layer.weight.data.numpy().shape)//3\n",
    "                decomposed = cp_decomposition_conv_layer(conv_layer, rank)\n",
    "            else:\n",
    "                print(\"Tucker Performed!\")\n",
    "                decomposed = tucker_decomposition_conv_layer(conv_layer)\n",
    "\n",
    "            model.visual_embedder.features._modules[key] = decomposed\n",
    "            \n",
    "    M = len(model.demographics_embedder.features._modules.keys())\n",
    "    for i, key in enumerate(model.demographics_embedder.features._modules.keys()):\n",
    "\n",
    "        if i >= M - 2:\n",
    "            break\n",
    "        if isinstance(model.demographics_embedder.features._modules[key], torch.nn.modules.conv.Conv2d):\n",
    "            conv_layer = model.demographics_embedder.features._modules[key]\n",
    "            if cp:\n",
    "                rank = max(conv_layer.weight.data.numpy().shape)//3\n",
    "                decomposed = cp_decomposition_conv_layer(conv_layer, rank)\n",
    "            else:\n",
    "                print(\"Tucker Performed!\")\n",
    "                decomposed = tucker_decomposition_conv_layer(conv_layer)\n",
    "\n",
    "            model.demographics_embedder.features._modules[key] = decomposed\n",
    "\n",
    "    torch.save(model, save_name)\n",
    "    print(\"Decomposed Model Saved!\")\n",
    "else:\n",
    "    print(\"Skipping Decomposition, Loading Decomposed Model from File.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9570c6d9",
   "metadata": {},
   "source": [
    "## Test Decomposed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c39e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestDecomposedModel():\n",
    "    concat_decomposed_model = torch.load('6aec_concat_mfn_R2_decomposed').to(device)\n",
    "    concat_decomposed_optimizer = optim.SGD(concat_medical_network.parameters(), lr=0.00007, momentum=0.70)\n",
    "    concat_decomposed_trainer = Trainer(datasets, concat_medical_network, optimizer)\n",
    "\n",
    "    _, _, _, d_classification_labels, d_classification_out = concat_decomposed_trainer.test()\n",
    "    \n",
    "#     print(d_classification_labels)\n",
    "#     print(d_classification_out)\n",
    "\n",
    "#     i = 0\n",
    "#     for patient in dataloaders['train']:\n",
    "#         i = i + 1\n",
    "#         if i > 5:\n",
    "#             break\n",
    "#         print(patient)\n",
    "\n",
    "    example_input, classes = next(iter(dataloaders['train'])) \n",
    "    example_input = (example_input[0].to(device), example_input[1].to(device), example_input[2].to(device))\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"----- Model Computational Complexity -----\")\n",
    "    # Calculate FLOPs\n",
    "    flops = torchprofile.profile_macs(concat_decomposed_model, example_input)\n",
    "    print(f'FLOPs: {flops}')\n",
    "\n",
    "#     decomposed_macs = calculate_flop(concat_decomposed_model)\n",
    "    \n",
    "TestDecomposedModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92f60c",
   "metadata": {},
   "source": [
    "# Preprocessing Features for Cox Survivability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fae96e",
   "metadata": {},
   "source": [
    "#### New Patient Dataset & Dataloder containing all Patients\n",
    "Subject_ID is now also returned in the label for constructing feature dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID_Patient_Dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe # Reference the MIMIC-IV Dataframe.\n",
    "        self.transform = transform # Apply any given transformations.\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index] # Get the row (patient) we want to read.\n",
    "\n",
    "        image = Image.open(row['image_path'])\n",
    "        if self.transform :\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        los = torch.tensor(row['los'], dtype=torch.float32)\n",
    "        expire_flag = torch.tensor(row['hospital_expire_flag'], dtype=torch.float32)\n",
    "        subject_id = torch.tensor(row['subject_id'], dtype=torch.float32)\n",
    "        stay_id = torch.tensor(row['stay_id'], dtype=torch.float32)\n",
    "        label = (los, expire_flag, subject_id, stay_id)\n",
    "        \n",
    "        demographics = torch.tensor(row['one_hot'], dtype=torch.float32)\n",
    "    \n",
    "        time_series = pd.read_csv(row['time_series_path'])\n",
    "    \n",
    "        # Drop charttime, since it isn't a feature\n",
    "        time_series = time_series.drop('charttime', axis=1)\n",
    "\n",
    "        # perform imputation\n",
    "        time_series = time_series.ffill()\n",
    "        time_series = time_series.bfill()\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        time_series_tensor = torch.tensor(time_series.values, dtype=torch.float32)\n",
    "        \n",
    "        patient = (image, demographics, time_series_tensor)\n",
    "        \n",
    "        return patient, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685cc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "Total_ID_Dataset = ID_Patient_Dataset(mimic_df, data_transforms['test'])\n",
    "Total_ID_Dataset.transform = data_transforms['test']\n",
    "Total_ID_Dataloader = DataLoader(Total_ID_Dataset, batch_size, shuffle=False)\n",
    "\n",
    "# i = 0\n",
    "# for patient in Total_ID_Dataloader:\n",
    "#     i = i + 1\n",
    "#     if i > 5:\n",
    "#         break\n",
    "#     print(patient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e74217",
   "metadata": {},
   "source": [
    "#### Apply TFN Network for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647dcba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patient_features():\n",
    "    processing_tfn_model = torch.load('6aec_concat_mfn_R2').to(device)\n",
    "\n",
    "    feature_list = []\n",
    "\n",
    "    for i, (entry) in enumerate(Total_ID_Dataloader):\n",
    "        patient = entry[0]\n",
    "        patient = (patient[0].to(device), patient[1].to(device), patient[2].to(device))\n",
    "        label = entry[1]\n",
    "        label = [label[0].item(), label[1].item(), label[2].item(), label[3].item()]\n",
    "        label[1] = int(label[1])\n",
    "        label[2] = int(label[2])\n",
    "        label[3] = int(label[3])\n",
    "\n",
    "        # get features, put in panda\n",
    "        output = processing_tfn_model.forward(patient)\n",
    "\n",
    "        features = output[0]\n",
    "        features = features.tolist()\n",
    "        features = features + label\n",
    "\n",
    "        feature_list.append(features)\n",
    "\n",
    "    feature_dataframe = pd.DataFrame(feature_list)\n",
    "    feature_dataframe.rename(columns={4122: 'duration', 4123: 'event', 4124: 'subject_id',\\\n",
    "                                      4125: 'stay_id'}, inplace=True)\n",
    "\n",
    "    feature_dataframe.drop_duplicates(subset='stay_id', keep='first', inplace=True)\n",
    "    feature_dataframe.to_csv('fusionmfn_patient_features.csv', index=False)\n",
    "\n",
    "    feature_dataframe\n",
    "    \n",
    "extract_patient_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02923591",
   "metadata": {},
   "source": [
    "# Survivability Analysis utilizing Cox Survivability Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6e004",
   "metadata": {},
   "source": [
    "#### Load Data into Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_train = pd.read_csv('patient_features.csv')\n",
    "cox_train.drop(['subject_id', 'stay_id'], axis=1, inplace=True)\n",
    "cox_train.columns = cox_train.columns.astype(str)\n",
    "# 0 represents survival/censured data, and 1 represents mortality, so flipping is not necessary.\n",
    "cox_train['event'] = cox_train['event'].replace({0:1, 1:0})\n",
    "cox_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b95d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_train['event'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b97159",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_test = cox_train.sample(frac=0.2)\n",
    "cox_train = cox_train.drop(cox_test.index)\n",
    "cox_val = cox_train.sample(frac=0.2)\n",
    "cox_train = cox_train.drop(cox_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2968a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_standardize = []\n",
    "cols_leave = list(cox_test.columns)\n",
    "del cols_leave[-2:]\n",
    "# cols_leave = list(map(int, cols_leave)) DON'T convert to ints... we don't want this!\n",
    "\n",
    "standardize = [([col], StandardScaler()) for col in cols_standardize]\n",
    "leave = [(col, None) for col in cols_leave]\n",
    "\n",
    "x_mapper = DataFrameMapper(standardize + leave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_mapper.fit_transform(cox_train).astype('float32')\n",
    "x_val = x_mapper.transform(cox_val).astype('float32')\n",
    "x_test = x_mapper.transform(cox_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e584e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "y_train = get_target(cox_train)\n",
    "y_val = get_target(cox_val)\n",
    "durations_test, events_test = get_target(cox_test)\n",
    "val = tt.tuplefy(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b244cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val.repeat(2).cat().shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c4d14",
   "metadata": {},
   "source": [
    "#### Cox-CC Input Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38849c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = x_train.shape[1]\n",
    "num_nodes = [32, 32]\n",
    "out_features = 1\n",
    "batch_norm = True\n",
    "dropout = 0.1\n",
    "output_bias = False\n",
    "\n",
    "cox_net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm,\n",
    "                              dropout, output_bias=output_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32beeed",
   "metadata": {},
   "source": [
    "#### Cox-CC Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb9b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke Cox model by wrapping it around MLP.\n",
    "cox_model = CoxCC(cox_net, optimizer=tt.optim.Adam, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e9e88b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "lrfinder = cox_model.lr_finder(x_train, y_train, batch_size, tolerance=2)\n",
    "_ = lrfinder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d1c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfinder.get_best_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_model.optimizer.set_lr(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 512\n",
    "epochs = 120\n",
    "# callbacks = [tt.callbacks.EarlyStopping()]\n",
    "callbacks = []\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = False\n",
    "\n",
    "if fit_model:\n",
    "    %%time\n",
    "    log = cox_model.fit(x_train, y_train, batch_size, epochs, callbacks, verbose,\n",
    "                    val_data=val.repeat(10).cat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fit_model:\n",
    "    _ = log.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d59e35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cox_model.partial_log_likelihood(*val).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c11514f",
   "metadata": {},
   "source": [
    "## Serialize/Deserialize Cox Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize = False\n",
    "\n",
    "if serialize:\n",
    "    cox_model.save_net('6en_cox_invert_flag')\n",
    "else:\n",
    "    cox_model.load_net('6en_cox_invert_flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563bb09",
   "metadata": {},
   "source": [
    "## Cox-CC Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b64a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cox_model.compute_baseline_hazards()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1deef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = cox_model.predict_surv_df(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376eb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv.iloc[:, :10].plot()\n",
    "plt.ylabel('S(t | x)')\n",
    "_ = plt.xlabel('Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f6dd1",
   "metadata": {},
   "source": [
    "These are the individual predictions of patients within cox_test, printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c0a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafb34b5",
   "metadata": {},
   "source": [
    "# Cox-CC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c907029",
   "metadata": {},
   "source": [
    "#### Concordance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b94090",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.concordance_td()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356070fc",
   "metadata": {},
   "source": [
    "#### Brier Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7decd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "ev.brier_score(time_grid).plot()\n",
    "plt.ylabel('Brier score')\n",
    "_ = plt.xlabel('Time')\n",
    "plt.savefig('brier_time.png',format='png',dpi=1200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7c59e",
   "metadata": {},
   "source": [
    "#### Negative binomial log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d139a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.nbll(time_grid).plot()\n",
    "plt.ylabel('Negative Binomial Log-Likelihood')\n",
    "_ = plt.xlabel('Time')\n",
    "plt.savefig('nbll_time.png',format='png',dpi=1200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e57f0",
   "metadata": {},
   "source": [
    "#### Integrated Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cdc4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.integrated_brier_score(time_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.integrated_nbll(time_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev[2:3].plot_surv()\n",
    "plt.ylabel('Survival')\n",
    "_ = plt.xlabel('Time')\n",
    "\n",
    "plt.savefig('example_survival_curve.png',format='png',dpi=1200,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995e1e2",
   "metadata": {},
   "source": [
    "# Compare Cox Model against Random Survival Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame with shape (1676, 4124)\n",
    "patient_df = pd.read_csv('patient_features.csv')\n",
    "\n",
    "# Separate features, duration, and event columns\n",
    "X = patient_df.iloc[:, :-2].values  # First 4121 columns as features\n",
    "durations = patient_df.iloc[:, -2].values  # Second to last column as duration\n",
    "events = patient_df.iloc[:, -1].values  # Last column as event marker\n",
    "\n",
    "# Create structured array for survival data\n",
    "y = np.array([(e, t) for e, t in zip(events, durations)], dtype=[('event', 'bool'), ('time', 'float')])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Survival Forest\n",
    "rsf = RandomSurvivalForest(n_estimators=100, min_samples_split=10, min_samples_leaf=15, random_state=42)\n",
    "rsf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dccd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get concordance score.\n",
    "rsf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe6347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions for evaluation\n",
    "y_train_pred = rsf.predict_survival_function(X_train)\n",
    "y_test_pred = rsf.predict_survival_function(X_test)\n",
    "\n",
    "# Adjust the time grid to be within the observed durations of the test set\n",
    "min_time_test = y_test['time'].min()\n",
    "max_time_test = y_test['time'].max()\n",
    "time_grid = np.linspace(min_time_test, max_time_test, 100)\n",
    "\n",
    "# Convert the step functions to arrays\n",
    "y_train_pred = np.asarray([[fn(t) for t in time_grid] for fn in y_train_pred])\n",
    "y_test_pred = np.asarray([[fn(t) for t in time_grid] for fn in y_test_pred])\n",
    "\n",
    "# Evaluate Performance\n",
    "c_index_train = concordance_index_censored(y_train['event'], y_train['time'], rsf.predict(X_train))[0]\n",
    "c_index_test = concordance_index_censored(y_test['event'], y_test['time'], rsf.predict(X_test))[0]\n",
    "\n",
    "print(f'C-Index (Train): {c_index_train}')\n",
    "print(f'C-Index (Test): {c_index_test}')\n",
    "\n",
    "# FIXME: IBS Calculation is broken!!!\n",
    "# Calculate Integrated Brier Score (IBS)\n",
    "# ibs_train = integrated_brier_score(y_train, y_train, y_train_pred, time_grid)\n",
    "# ibs_test = integrated_brier_score(y_train, y_test, y_test_pred, time_grid)\n",
    "\n",
    "# print(f'Integrated Brier Score (Train): {ibs_train}')\n",
    "# print(f'Integrated Brier Score (Test): {ibs_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bb93ed",
   "metadata": {},
   "source": [
    "# Misc Testbenching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tfn_model = torch.load('6aec_concat_mfn_decomposed').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11abe274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader):\n",
    "        model.eval()\n",
    "        total = 0\n",
    "        total_time = 0\n",
    "        preds = []\n",
    "        regression_labels = []\n",
    "        classification_labels = []\n",
    "        regression_outputs = []\n",
    "        classification_outputs = []\n",
    "        \n",
    "        for i, (entry) in enumerate(dataloader):\n",
    "            # Get image, demographics, and label from dataloader and send to device.\n",
    "            patient = entry[0]\n",
    "            patient = (patient[0].to(device), patient[1].to(device), patient[2].to(device))\n",
    "            label = entry[1]\n",
    "            label = (label[0].to(device), label[1].to(device))\n",
    "            current_regression_label = label[0].to(device)\n",
    "            current_classification_label = label[1].to(device)\n",
    "\n",
    "            # Add each label to list keeping track of all entries.\n",
    "            regression_labels.extend(current_regression_label.to(\"cpu\"))\n",
    "            classification_labels.extend(current_classification_label.to(\"cpu\"))\n",
    "\n",
    "            # Start keeping time, and run model for output.\n",
    "            t0 = time.time()\n",
    "            \n",
    "            output = model.regression_classification(patient)\n",
    "            \n",
    "            t1 = time.time()\n",
    "            total_time = total_time + (t1 - t0)\n",
    "            \n",
    "            # Add model outputs to output lists. (For later comparison)\n",
    "            for output_tensor in output[0]:\n",
    "#                 print(\"Regression:\", output_tensor)\n",
    "                regression_outputs.extend(output_tensor.to(\"cpu\").detach().numpy().tolist())\n",
    "                \n",
    "            for output_tensor in output[1]:\n",
    "#                 print(\"Classification:\", output_tensor)\n",
    "                classification_outputs.extend(output_tensor.to(\"cpu\").detach().numpy().tolist())\n",
    "        \n",
    "        # Print model training time and statistics.\n",
    "        print(\"=== Regression Accuracy ===\")\n",
    "        print(\"Mean Squared Error:\", mean_squared_error(regression_labels, regression_outputs))\n",
    "        print(\"=== Classification Accuracy ===\")\n",
    "        classification_outputs_rounded = [ round(elem) for elem in classification_outputs ]\n",
    "#         print(classification_outputs_rounded)\n",
    "        print(\"Accuracy Score:\", accuracy_score(classification_labels, classification_outputs_rounded))\n",
    "        calc_time = float(total_time) / (i + 1)\n",
    "        print(\"Total Prediction Time:\", total_time)\n",
    "        print('Average Prediction Time: {min}m {sec}s'.format(min=calc_time // 60.0, sec=calc_time % 60.0))\n",
    "        print(\"Total Entries Compared: \", i + 1)\n",
    "#         print(outputs)\n",
    "        \n",
    "        return (total_time, regression_labels, regression_outputs, \\\n",
    "                classification_labels, classification_outputs_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a252c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eval_optimizer = optim.SGD(eval_tfn_model.parameters(), lr=0.00007, momentum=0.70)\n",
    "# eval_trainer = Trainer(datasets, eval_tfn_model, eval_optimizer)\n",
    "\n",
    "concat_time, concat_regression_labels, concat_regression_outputs, \\\n",
    "concat_classification_labels, concat_classification_outputs = test_model(eval_tfn_model, Total_ID_Dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_classification_labels = [int(x.item()) for x in concat_classification_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_classification_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think there may be not enough mortalities in the dataset, \n",
    "# so the model swings too far and marks every patient as a survival.\n",
    "# I'll have to look at the classification again because it seems it might not be learning the patterns correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original():\n",
    "    label_names = [\"Censored survival\", \"Mortality\"]\n",
    "    plot_confusion_matrix(confusion_matrix(concat_classification_labels, \\\n",
    "                                           concat_classification_outputs), label_names)\n",
    "\n",
    "    #plt.savefig('original_cm.svg',format='svg',dpi=1200,bbox_inches='tight')\n",
    "    plt.savefig('classification_cm.png',format='png',dpi=1200,bbox_inches='tight')\n",
    "    \n",
    "plot_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "# print(original_trainer.get_belief())\n",
    "\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(concat_classification_labels, \\\n",
    "                                                 concat_classification_outputs)\n",
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, color='blue', label='Original Model (AUC = {:.2f})'.format(roc_auc))\n",
    "\n",
    "# fpr, tpr, thresholds = sklearn.metrics.roc_curve(decomposed_labels, decomposed_outputs[1::2])\n",
    "# roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "# plt.plot(fpr, tpr, color='green', label='Decomposed Model (AUC = {:.2f})'.format(roc_auc))\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bacd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LOSresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
