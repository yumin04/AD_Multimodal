{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch vs Keras VGG Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- PYTORCH -----\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import sys\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import time\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import partial_tucker\n",
    "from decompositions import cp_decomposition_conv_layer, tucker_decomposition_conv_layer\n",
    "\n",
    "from VBMF import VBMF\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os, os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import asarray\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "from sksurv.datasets import load_gbsg2\n",
    "# from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_censored, concordance_index_ipcw, integrated_brier_score\n",
    "\n",
    "import itertools\n",
    "from itertools import *\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "# # ----- KERAS -----\n",
    "# import keras\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.vgg16 import preprocess_input\n",
    "# import numpy as np\n",
    "\n",
    "# import time\n",
    "\n",
    "# import os\n",
    "\n",
    "# import PIL.Image as Image\n",
    "# import matplotlib.pylab as plt\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow.keras.layers as tfl\n",
    "\n",
    "# # import misc\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# import datetime\n",
    "\n",
    "# import glob\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "# from tensorflow.keras import utils\n",
    "\n",
    "# import keras\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # from tensorflow.keras.utils import ImageDataGenerator, img_to_array, load_img\n",
    "# from tensorflow.keras.layers import AveragePooling2D, Dropout, Flatten, Dense, Input\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "# # from keras.utils import np_utils\n",
    "# from tensorflow.keras.optimizers import RMSprop\n",
    "# from tensorflow.keras.layers import Concatenate\n",
    "# from sklearn.datasets import load_files\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# # import tensorflow_datasets as tfds\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if possible\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use PyTorch as Tensorly Backend\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Dataloader and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset\n",
    "dataset_path = \"/home/mason/ADNI_Dataset/ADNI_IMG_32.5%_x_organized\"\n",
    "\n",
    "# Data transforms (resize, convert to tensor, normalize)\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),         # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedVGG16Model(torch.nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(ModifiedVGG16Model, self).__init__()\n",
    "\n",
    "        model = models.vgg16(weights='IMAGENET1K_V1')\n",
    "        self.features = model.features\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)  # GlobalAveragePooling2D\n",
    "        \n",
    "        self.shared = nn.Sequential( # Try simplififying these layers!\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 256))\n",
    "        \n",
    "        # Contains the Tail of VGG16 (all 3 FC layers and ReLU, when combined with embedder)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.shared(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModifiedVGG16Model()\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 1.2283, Accuracy: 0.4172\n",
      "Validation Loss: 1.0660, Validation Accuracy: 0.4577\n",
      "Epoch 2/30, Loss: 1.0620, Accuracy: 0.4476\n",
      "Validation Loss: 1.0475, Validation Accuracy: 0.4577\n",
      "Epoch 3/30, Loss: 1.0567, Accuracy: 0.4476\n",
      "Validation Loss: 1.0473, Validation Accuracy: 0.4577\n",
      "Epoch 4/30, Loss: 1.0558, Accuracy: 0.4476\n",
      "Validation Loss: 1.0492, Validation Accuracy: 0.4577\n",
      "Epoch 5/30, Loss: 1.0568, Accuracy: 0.4476\n",
      "Validation Loss: 1.0483, Validation Accuracy: 0.4577\n",
      "Epoch 6/30, Loss: 1.0576, Accuracy: 0.4424\n",
      "Validation Loss: 1.0473, Validation Accuracy: 0.4577\n",
      "Epoch 7/30, Loss: 1.0556, Accuracy: 0.4476\n",
      "Validation Loss: 1.0472, Validation Accuracy: 0.4577\n",
      "Epoch 8/30, Loss: 1.0554, Accuracy: 0.4476\n",
      "Validation Loss: 1.0471, Validation Accuracy: 0.4577\n",
      "Epoch 9/30, Loss: 1.0562, Accuracy: 0.4476\n",
      "Validation Loss: 1.0456, Validation Accuracy: 0.4577\n",
      "Epoch 10/30, Loss: 1.0547, Accuracy: 0.4476\n",
      "Validation Loss: 1.0465, Validation Accuracy: 0.4577\n",
      "Epoch 11/30, Loss: 1.0572, Accuracy: 0.4458\n",
      "Validation Loss: 1.0469, Validation Accuracy: 0.4577\n",
      "Epoch 12/30, Loss: 1.0552, Accuracy: 0.4476\n",
      "Validation Loss: 1.0473, Validation Accuracy: 0.4577\n",
      "Epoch 13/30, Loss: 1.0660, Accuracy: 0.4476\n",
      "Validation Loss: 1.0484, Validation Accuracy: 0.4577\n",
      "Epoch 14/30, Loss: 1.0542, Accuracy: 0.4476\n",
      "Validation Loss: 1.0474, Validation Accuracy: 0.4577\n",
      "Epoch 15/30, Loss: 1.0562, Accuracy: 0.4476\n",
      "Validation Loss: 1.0480, Validation Accuracy: 0.4577\n",
      "Epoch 16/30, Loss: 1.0550, Accuracy: 0.4476\n",
      "Validation Loss: 1.0469, Validation Accuracy: 0.4577\n",
      "Epoch 17/30, Loss: 1.0556, Accuracy: 0.4476\n",
      "Validation Loss: 1.0499, Validation Accuracy: 0.4577\n",
      "Epoch 18/30, Loss: 1.0565, Accuracy: 0.4476\n",
      "Validation Loss: 1.0472, Validation Accuracy: 0.4577\n",
      "Epoch 19/30, Loss: 1.0556, Accuracy: 0.4476\n",
      "Validation Loss: 1.0476, Validation Accuracy: 0.4577\n",
      "Epoch 20/30, Loss: 1.0555, Accuracy: 0.4476\n",
      "Validation Loss: 1.0468, Validation Accuracy: 0.4577\n",
      "Epoch 21/30, Loss: 1.0556, Accuracy: 0.4476\n",
      "Validation Loss: 1.0470, Validation Accuracy: 0.4577\n",
      "Epoch 22/30, Loss: 1.0562, Accuracy: 0.4476\n",
      "Validation Loss: 1.0477, Validation Accuracy: 0.4577\n",
      "Epoch 23/30, Loss: 1.0569, Accuracy: 0.4476\n",
      "Validation Loss: 1.0483, Validation Accuracy: 0.4577\n",
      "Epoch 24/30, Loss: 1.0555, Accuracy: 0.4476\n",
      "Validation Loss: 1.0478, Validation Accuracy: 0.4577\n",
      "Epoch 25/30, Loss: 1.0567, Accuracy: 0.4476\n",
      "Validation Loss: 1.0473, Validation Accuracy: 0.4577\n",
      "Epoch 26/30, Loss: 1.0557, Accuracy: 0.4476\n",
      "Validation Loss: 1.0472, Validation Accuracy: 0.4577\n",
      "Epoch 27/30, Loss: 1.0550, Accuracy: 0.4476\n",
      "Validation Loss: 1.0473, Validation Accuracy: 0.4577\n",
      "Epoch 28/30, Loss: 1.0555, Accuracy: 0.4476\n",
      "Validation Loss: 1.0472, Validation Accuracy: 0.4577\n",
      "Epoch 29/30, Loss: 1.0550, Accuracy: 0.4476\n",
      "Validation Loss: 1.0477, Validation Accuracy: 0.4577\n",
      "Epoch 30/30, Loss: 1.0562, Accuracy: 0.4476\n",
      "Validation Loss: 1.0469, Validation Accuracy: 0.4577\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Print training stats\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {train_loss/len(train_loader):.4f}, Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_correct/val_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LOSresearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
