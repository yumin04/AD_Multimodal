{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7bdeb29",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0daccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import sys\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import time\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import partial_tucker\n",
    "from decompositions import cp_decomposition_conv_layer, tucker_decomposition_conv_layer\n",
    "\n",
    "from VBMF import VBMF\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os, os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import asarray\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "from sksurv.datasets import load_gbsg2\n",
    "# from sksurv.preprocessing import OneHotEncoder\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import concordance_index_censored, concordance_index_ipcw, integrated_brier_score\n",
    "\n",
    "import itertools\n",
    "from itertools import *\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import torchtuples as tt\n",
    "from pycox.models import CoxCC\n",
    "from pycox.utils import kaplan_meier\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from ptflops import get_model_complexity_info\n",
    "import torchprofile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148825a",
   "metadata": {},
   "source": [
    "# Define PyTorch Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc042c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if possible\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ^ Usually cuda:0, but at time of writing all avaliable memory on GPU 0 is in use.\n",
    "\n",
    "# Use PyTorch as Tensorly Backend\n",
    "tl.set_backend('pytorch')\n",
    "\n",
    "# Force CPU Evaluation (Not Recommended)\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5079f",
   "metadata": {},
   "source": [
    "# Deserialize MIMIC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b30be462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PTID</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>053_S_1044</td>\n",
       "      <td>patients_csv/053_S_1044.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>035_S_0204</td>\n",
       "      <td>patients_csv/035_S_0204.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>027_S_0256</td>\n",
       "      <td>patients_csv/027_S_0256.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128_S_0230</td>\n",
       "      <td>patients_csv/128_S_0230.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114_S_0173</td>\n",
       "      <td>patients_csv/114_S_0173.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>029_S_0843</td>\n",
       "      <td>patients_csv/029_S_0843.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>129_S_1204</td>\n",
       "      <td>patients_csv/129_S_1204.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>032_S_1169</td>\n",
       "      <td>patients_csv/032_S_1169.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>099_S_0060</td>\n",
       "      <td>patients_csv/099_S_0060.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>013_S_0860</td>\n",
       "      <td>patients_csv/013_S_0860.pkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>815 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PTID                         path\n",
       "0    053_S_1044  patients_csv/053_S_1044.pkl\n",
       "1    035_S_0204  patients_csv/035_S_0204.pkl\n",
       "2    027_S_0256  patients_csv/027_S_0256.pkl\n",
       "3    128_S_0230  patients_csv/128_S_0230.pkl\n",
       "4    114_S_0173  patients_csv/114_S_0173.pkl\n",
       "..          ...                          ...\n",
       "810  029_S_0843  patients_csv/029_S_0843.pkl\n",
       "811  129_S_1204  patients_csv/129_S_1204.pkl\n",
       "812  032_S_1169  patients_csv/032_S_1169.pkl\n",
       "813  099_S_0060  patients_csv/099_S_0060.pkl\n",
       "814  013_S_0860  patients_csv/013_S_0860.pkl\n",
       "\n",
       "[815 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = None\n",
    "\n",
    "ad_patient_df = pd.read_csv(\"AD_Patient_Manifest.csv\")\n",
    "\n",
    "ad_patient_df.reset_index(drop=True)\n",
    "\n",
    "ad_patient_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d49a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652\n",
      "163\n"
     ]
    }
   ],
   "source": [
    "# Split MIMIC-IV Dataset into 80-20% for training and testing.\n",
    "train = ad_patient_df.sample(frac=0.8,random_state=200)\n",
    "test = ad_patient_df.drop(train.index)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "# train\n",
    "# train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cabee20",
   "metadata": {},
   "source": [
    "# Define Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc6639",
   "metadata": {},
   "source": [
    "### Patient Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29c8b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CustomDataset gets ADNI cohorts for a NN.\n",
    "class Patient_Dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.manifest = dataframe\n",
    "        self.transform = transform # Apply any given transformations.\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row_entry = self.manifest.iloc[index]['path'] # Get the row (patient) we want to read.\n",
    "        cohort = pd.read_pickle(row_entry)\n",
    "\n",
    "        image = Image.open(cohort.iloc[0]['image_path'])\n",
    "        if self.transform :\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # For Cox Model, label should be in the tuple: duration, event\n",
    "        mmse = torch.tensor(cohort.iloc[0]['MMSE'], dtype=torch.float32)\n",
    "        dx = torch.tensor(cohort.iloc[0]['DX_encoded'], dtype=torch.float32)\n",
    "        label = (mmse, dx)\n",
    "        \n",
    "        demographics = torch.tensor(cohort.iloc[0]['one_hot_vector'], dtype=torch.float32)\n",
    "    \n",
    "        time_series = cohort[['Years_bl', 'ADAS11', 'ADAS13', 'ADASQ4']]\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        time_series_tensor = torch.tensor(time_series.values, dtype=torch.float32)\n",
    "        \n",
    "        patient = (image, demographics, time_series_tensor)\n",
    "        \n",
    "        return patient, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b64ab0",
   "metadata": {},
   "source": [
    "### Patient Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10c4c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]), tensor([[0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]]), tensor([[[ 0.0000,  6.3300, 11.3300,  4.0000],\n",
      "         [ 0.5339,  6.3300, 12.3300,  5.0000],\n",
      "         [ 1.2594,  7.0000, 13.0000,  5.0000],\n",
      "         [ 2.0233,  5.3300,  8.3300,  3.0000]]])], [tensor([30.]), tensor([0.])]]\n",
      "[[tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]), tensor([[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.]]), tensor([[[ 0.0000,  9.6700, 14.6700,  5.0000],\n",
      "         [ 0.4736,  9.0000, 16.0000,  7.0000],\n",
      "         [ 1.0157, 11.0000, 18.0000,  7.0000],\n",
      "         [ 1.4894,  8.0000, 14.0000,  6.0000],\n",
      "         [ 2.0342, 14.0000, 24.0000, 10.0000],\n",
      "         [ 3.0417, 13.3300, 22.3300,  9.0000],\n",
      "         [ 4.0438, 10.3300, 19.3300,  9.0000],\n",
      "         [ 5.0185, 19.0000, 29.0000, 10.0000],\n",
      "         [ 5.9795, 19.0000, 29.0000, 10.0000],\n",
      "         [ 7.1294, 28.0000, 39.0000, 10.0000],\n",
      "         [ 7.9836, 28.0000, 38.0000, 10.0000],\n",
      "         [ 9.1006, 30.0000, 43.0000, 10.0000],\n",
      "         [10.1027, 31.0000, 43.0000, 10.0000]]])], [tensor([25.]), tensor([2.])]]\n",
      "[[tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]), tensor([[0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]]), tensor([[[ 0.0000,  2.6700,  2.6700,  0.0000],\n",
      "         [ 0.6680,  1.0000,  1.0000,  0.0000],\n",
      "         [ 1.0021, 12.6700, 13.6700,  1.0000],\n",
      "         [ 1.9603,  8.3300, 10.3300,  1.0000],\n",
      "         [ 3.0144,  5.6700,  5.6700,  0.0000],\n",
      "         [ 4.0055,  2.6700,  3.6700,  1.0000],\n",
      "         [ 5.0212,  4.0000,  6.0000,  1.0000],\n",
      "         [ 6.0178,  3.0000,  4.0000,  1.0000],\n",
      "         [ 6.9952,  2.0000,  3.0000,  1.0000],\n",
      "         [ 7.9699,  2.0000,  3.0000,  1.0000],\n",
      "         [ 9.9822,  5.0000,  6.0000,  1.0000]]])], [tensor([29.]), tensor([0.])]]\n",
      "[[tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]), tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]]), tensor([[[ 0.0000,  8.6700, 16.6700,  7.0000],\n",
      "         [ 0.6708, 11.3300, 18.3300,  6.0000],\n",
      "         [ 1.0678, 11.6700, 17.6700,  6.0000]]])], [tensor([29.]), tensor([2.])]]\n",
      "[[tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]]), tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.]]), tensor([[[ 0.0000,  8.3300, 10.3300,  2.0000],\n",
      "         [ 0.4846,  6.3300,  7.3300,  1.0000],\n",
      "         [ 1.0048,  4.0000,  6.0000,  2.0000],\n",
      "         [ 1.9986,  2.6700,  3.6700,  1.0000],\n",
      "         [ 2.9952,  6.3300,  9.3300,  3.0000],\n",
      "         [ 3.9754,  8.3300,  9.3300,  1.0000],\n",
      "         [ 5.0075,  9.0000, 11.0000,  2.0000],\n",
      "         [ 6.0151, 12.0000, 14.0000,  2.0000],\n",
      "         [ 6.9623, 10.0000, 13.0000,  3.0000],\n",
      "         [ 8.0329,  6.0000, 11.0000,  5.0000],\n",
      "         [ 9.0349, 18.0000, 25.0000,  6.0000],\n",
      "         [10.0397, 22.0000, 29.0000,  6.0000],\n",
      "         [11.4853, 45.0000, 58.0000,  9.0000],\n",
      "         [12.5886, 55.3300, 68.3300,  8.0000]]])], [tensor([30.]), tensor([0.])]]\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "# Will need to be applied by passing in to Dataset constructor!\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "#         torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "#         transforms.RandomResizedCrop(224),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5), (0.5))\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "#         torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "#         transforms.Resize(256),\n",
    "#         transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.5), (0.5))\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "dl_args = dict(batch_size=16, num_workers=4)\n",
    "\n",
    "# Dataloaders used to iterate through the patients. Patients split 80-20% into train-test loaders.\n",
    "train_dataset = Patient_Dataset(train, data_transforms['train'])\n",
    "train_dataset.transform = data_transforms['train']\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "\n",
    "test_dataset = Patient_Dataset(test, data_transforms['test'])\n",
    "test_dataset.transform = data_transforms['test']\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "dataloaders = {'train': train_dataloader,\n",
    "              'test': test_dataloader,\n",
    "              }\n",
    "\n",
    "i = 0\n",
    "for patient in dataloaders['train']:\n",
    "    i = i + 1\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(patient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31050867",
   "metadata": {},
   "source": [
    "# Define Independent Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25462372",
   "metadata": {},
   "source": [
    "Independent evaluation of LOS regression performance and mortality classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f5834",
   "metadata": {},
   "source": [
    "### X-Ray Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b102cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedVGG16Model(torch.nn.Module):\n",
    "    def __init__(self, model=None):\n",
    "        super(ModifiedVGG16Model, self).__init__()\n",
    "\n",
    "        model = models.vgg16(weights='IMAGENET1K_V1')\n",
    "        self.features = model.features\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        \n",
    "        # When embedding, we only want the output of the first FC layer.\n",
    "#         self.shared = nn.Sequential( # Try simplififying these layers!\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(25088, 4096),\n",
    "#             nn.ReLU(inplace=True)) # This last ReLU layer may be unnecessary\n",
    "        \n",
    "        self.shared = nn.Sequential( # Try simplififying these layers!\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True)) # This last ReLU layer may be unnecessary\n",
    "        \n",
    "        # Apply last FC layer for regression.\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(4096, 1))\n",
    "        \n",
    "        # Contains the Tail of VGG16 (all 3 FC layers and ReLU, when combined with embedder)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4096, 3))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.shared(x)\n",
    "        los = self.regressor(x)\n",
    "        mortality = self.classifier(x)\n",
    "\n",
    "        return (los, mortality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679506e0",
   "metadata": {},
   "source": [
    "### Demographics Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d429a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemographicsEmbedder(torch.nn.Module):\n",
    "    def __init__(self, model=None):\n",
    "        super(DemographicsEmbedder, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(1, 10, 3), # Fix input size!\n",
    "            nn.MaxPool1d(2))\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(140, 70),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(70, 10),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "        # FIXME: Update classifier to handle embedding and Conv1D output.\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(10, 1))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(10, 3))\n",
    "        \n",
    "    def forward(self, demographics):\n",
    "        x = demographics\n",
    "        x = x.view(1, 1, 30) # Reshape tensor to [N, C, L] format expected by Conv1D\n",
    "        x = self.features(x)\n",
    "        x = self.shared(x)\n",
    "        los = self.regressor(x)\n",
    "        mortality = self.classifier(x)\n",
    "        return (los, mortality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1ade5",
   "metadata": {},
   "source": [
    "### Time Series Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80f95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time_Series_Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size):\n",
    "        super(Time_Series_Autoencoder, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.hidden_to_latent = nn.Linear(hidden_size, latent_size)\n",
    "        self.latent_to_hidden = nn.Linear(latent_size, hidden_size)\n",
    "        self.decoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "#         self.decoder = nn.LSTM(hidden_size, input_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)  # Additional final linear layer\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequence\n",
    "        packed_x = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Encoder\n",
    "        packed_h, (h, c) = self.encoder(packed_x)\n",
    "        h = h[-1]  # Get the hidden state from the last layer of the LSTM\n",
    "        latent = self.hidden_to_latent(h)\n",
    "        \n",
    "        # Prepare for decoder\n",
    "        hidden = self.latent_to_hidden(latent).unsqueeze(0)\n",
    "        cell = torch.zeros_like(hidden)\n",
    "        \n",
    "        # Decoder\n",
    "        packed_output, _ = self.decoder(packed_x, (hidden, cell))\n",
    "        \n",
    "        # Pad the packed sequence\n",
    "        decoded, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Apply the final linear layer to map hidden state to input size\n",
    "        decoded = self.output_layer(decoded)\n",
    "        \n",
    "        return latent, decoded\n",
    "    \n",
    "    def encode(self, x, lengths):\n",
    "        with torch.no_grad():\n",
    "            # Pack the padded sequence\n",
    "            packed_x = rnn_utils.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "            \n",
    "            # Encoder\n",
    "            packed_h, (h, c) = self.encoder(packed_x)\n",
    "            h = h[-1]  # Get the hidden state from the last layer of the LSTM\n",
    "            latent = self.hidden_to_latent(h)\n",
    "        \n",
    "        return latent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b256c",
   "metadata": {},
   "source": [
    "# Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7122a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, dataloaders, model, optimizer, model_type):\n",
    "        self.optimizer = optimizer\n",
    "        self.model_type = model_type\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.classification_criterion = nn.CrossEntropyLoss()\n",
    "        self.regression_criterion = nn.MSELoss()\n",
    "        self.model.train()\n",
    "        self.train_regression_loss = []\n",
    "        self.train_classification_loss = []\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        total = 0\n",
    "        total_time = 0\n",
    "        running_regression_loss = 0.0\n",
    "        running_classification_loss = 0.0\n",
    "        classification_predictions = []\n",
    "        classification_labels = []\n",
    "        \n",
    "        for i, (entry) in enumerate(dataloaders['test']):\n",
    "            # Get image, demographics, and label from dataloader and send to device.\n",
    "            patient = entry[0]\n",
    "            patient = (patient[0].to(device), patient[1].to(device), patient[2].to(device))\n",
    "            \n",
    "            if self.model_type == \"x-ray\":\n",
    "                patient = patient[0].to(device)\n",
    "            if self.model_type == \"demographics\":\n",
    "                patient = patient[1].to(device)\n",
    "            if self.model_type == \"autoencoder\":\n",
    "                patient = patient[2].to(device)\n",
    "            \n",
    "            label = entry[1]\n",
    "            label = (label[0].to(device), label[1].to(device))\n",
    "            regression_label = label[0].to(device).unsqueeze(1)\n",
    "            classification_label = label[1].to(device)\n",
    "            classification_label = classification_label.long()\n",
    "            \n",
    "            # Start keeping time, and run model for output.\n",
    "            t0 = time.time()\n",
    "            \n",
    "            output = self.model.forward(patient)\n",
    "            \n",
    "            t1 = time.time()\n",
    "            total_time = total_time + (t1 - t0)\n",
    "            \n",
    "            # Calculate item loss\n",
    "            model_regression = output[0]\n",
    "            model_classification = output[1].float()\n",
    "            \n",
    "            # Keep track of current classification target\n",
    "            classification_labels.extend(classification_label.float().to(\"cpu\"))\n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = F.softmax(model_classification, dim=1)\n",
    "            # Use argmax to get the index of the class with the highest probability\n",
    "            predicted_class = torch.argmax(probabilities, dim=1)\n",
    "            # extend output tracker \n",
    "            classification_predictions.extend(predicted_class.to(\"cpu\").detach().numpy().tolist())\n",
    "            \n",
    "            regression_loss = self.regression_criterion(model_regression, regression_label)\n",
    "            classification_loss = self.classification_criterion(model_classification, classification_label)\n",
    "            \n",
    "            # Add to running total\n",
    "            running_regression_loss += regression_loss.item()\n",
    "            running_classification_loss += classification_loss.item()\n",
    "        \n",
    "        # Get average loss\n",
    "        total_regression_loss = running_regression_loss / len(dataloaders['test'])\n",
    "        total_classification_loss = running_classification_loss / len(dataloaders['test'])\n",
    "        \n",
    "        # Print model training time and statistics.\n",
    "#         print(\"=== Regression Accuracy ===\")\n",
    "#         print(\"Mean Squared Error:\", total_regression_loss)\n",
    "        print(\"MSE: \" + str(total_regression_loss), end=', ')\n",
    "#         print(\"=== Classification Accuracy ===\")\n",
    "        classification_labels = [ int(x.item()) for x in classification_labels ]\n",
    "        classification_predictions = [ round(elem) for elem in classification_predictions ]\n",
    "#         print(\"Accuracy Score:\", accuracy_score(classification_labels, classification_predictions))\n",
    "        print(\"ACC: \" + str(accuracy_score(classification_labels, classification_predictions))\n",
    "                                                          , end=', ')\n",
    "#         print(\"Cross Entropy Loss:\", total_classification_loss)\n",
    "        print(\"CEL:\", total_classification_loss)\n",
    "#         calc_time = float(total_time) / (i + 1)\n",
    "#         print(\"Total Prediction Time:\", total_time)\n",
    "#         print('Average Prediction Time: {min}m {sec}s'.format(min=calc_time // 60.0, sec=calc_time % 60.0))\n",
    "#         print(\"Total Entries Compared: \", i + 1)\n",
    "        \n",
    "        return (total_time, classification_labels, classification_predictions)\n",
    "\n",
    "    def train(self, epoches=10):\n",
    "        since = time.time()\n",
    "        self.train_regression_loss = []\n",
    "        self.train_classification_loss = []\n",
    "        \n",
    "        for i in range(epoches):\n",
    "            print(\"Epoch [\" + str(i) + \"/\" + str(epoches) + \"]\", end=', ')\n",
    "            self.train_epoch()\n",
    "            self.test()\n",
    "            self.model.eval()\n",
    "            \n",
    "        print(\"Finished fine tuning.\")\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in  {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        \n",
    "\n",
    "    def train_batch(self, patient, regression_label, classification_label):\n",
    "        self.model.train()\n",
    "        # Not sure why double works fine for forward, but not for backwards. .float() is here to fix this.\n",
    "\n",
    "        output = self.model.forward(patient)\n",
    "        model_regression = output[0]\n",
    "        model_classification = output[1].float()\n",
    "        classification_label = classification_label.squeeze(dim=0).long()\n",
    "        \n",
    "#         print(\"Classification Target:\", classification_label)\n",
    "#         # Apply softmax to get probabilities\n",
    "#         probabilities = F.softmax(model_classification, dim=1)\n",
    "#         # Use argmax to get the index of the class with the highest probability\n",
    "#         predicted_class = torch.argmax(probabilities, dim=1)\n",
    "#         print(\"Class Probabilities:  \", probabilities)\n",
    "#         print(\"Classification Pred:  \", predicted_class)\n",
    "#         print(\"\")\n",
    "        \n",
    "        regression_loss = self.regression_criterion(model_regression, regression_label)\n",
    "        \n",
    "        classification_loss = self.classification_criterion(model_classification, classification_label)\n",
    "        \n",
    "        main_weight = 0.0\n",
    "        aux_weight = 1.0\n",
    "        \n",
    "#         total_loss = (main_weight * regression_loss) + (aux_weight * classification_loss)\n",
    "\n",
    "        total_loss = classification_loss\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train_epoch(self):\n",
    "        for i, (entry) in enumerate(dataloaders['train']):\n",
    "            # NOTE: Disable model.to(device) for better traceback.\n",
    "            self.model.to(device)\n",
    "            patient = entry[0]\n",
    "            patient = (patient[0].to(device), patient[1].to(device), patient[2].to(device))\n",
    "            \n",
    "            if self.model_type == \"x-ray\":\n",
    "                patient = patient[0].to(device)\n",
    "            if self.model_type == \"demographics\":\n",
    "                patient = patient[1].to(device)\n",
    "            if self.model_type == \"autoencoder\":\n",
    "                patient = patient[2].to(device)\n",
    "            \n",
    "            label = entry[1]\n",
    "            label = (label[0].to(device), label[1].to(device))\n",
    "            regression_label = label[0].to(device)\n",
    "            classification_label = label[1].to(device)\n",
    "            # Convert input from [1] to [1, 1] size to match input.\n",
    "            regression_label = regression_label.unsqueeze(1)\n",
    "            classification_label = classification_label.unsqueeze(1)\n",
    "            self.train_batch(patient, regression_label, classification_label)\n",
    "#             if(i % 1000 == 0):\n",
    "#                 print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c55c31",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2906d",
   "metadata": {},
   "source": [
    "Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30b1619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ray_model = ModifiedVGG16Model()\n",
    "demographics_model = DemographicsEmbedder()\n",
    "autoencoder = torch.load(\"TFN_AEC_R1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d55a3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModifiedVGG16Model(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (shared): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (regressor): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=1, bias=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(x_ray_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c553a2",
   "metadata": {},
   "source": [
    "### X-Ray Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "547e2681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 725.251552090323, ACC: 0.294478527607362, CEL: 1.1020826415781595\n",
      "Epoch [0/10], MSE: 727.6307127782903, ACC: 0.4171779141104294, CEL: 2.4353040834443154\n",
      "Epoch [1/10], MSE: 727.4839088112299, ACC: 0.4171779141104294, CEL: 2.2245707547326394\n",
      "Epoch [2/10], MSE: 723.6910372306965, ACC: 0.4171779141104294, CEL: 2.545919853474008\n",
      "Epoch [3/10], MSE: 719.0707618385736, ACC: 0.4171779141104294, CEL: 2.3991910166512787\n",
      "Epoch [4/10], MSE: 717.6414366177986, ACC: 0.4171779141104294, CEL: 2.494465853538981\n",
      "Epoch [5/10], MSE: 714.900926461249, ACC: 0.4171779141104294, CEL: 2.5018783170112804\n",
      "Epoch [6/10], MSE: 714.9996601877037, ACC: 0.4171779141104294, CEL: 2.4611695296948124\n",
      "Epoch [7/10], MSE: 713.0093806916219, ACC: 0.4171779141104294, CEL: 2.668890069164564\n",
      "Epoch [8/10], MSE: 714.5622710245518, ACC: 0.4171779141104294, CEL: 2.553152663484673\n",
      "Epoch [9/10], MSE: 718.0495959322876, ACC: 0.4171779141104294, CEL: 2.562568790890688\n",
      "Finished fine tuning.\n",
      "Training complete in  2m 48s\n"
     ]
    }
   ],
   "source": [
    "x_ray_optimizer = torch.optim.Adam(x_ray_model.parameters(), lr=0.0001)\n",
    "x_ray_trainer = Trainer(datasets, x_ray_model, x_ray_optimizer, \"x-ray\")\n",
    "\n",
    "_, _, _ = x_ray_trainer.test()\n",
    "\n",
    "x_ray_trainer.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc98891",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "demographics_optimizer = torch.optim.Adam(demographics_model.parameters(), lr=0.0001)\n",
    "demographics_trainer = Trainer(datasets, demographics_model, demographics_optimizer, \"demographics\")\n",
    "\n",
    "_, _, _ = demographics_trainer.test()\n",
    "\n",
    "demographics_trainer.train(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b184f",
   "metadata": {},
   "source": [
    "# Autoencoder Custom Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92197aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.to(device)\n",
    "\n",
    "embed_list_train = []\n",
    "regression_labels_train = []\n",
    "classification_labels_train = []\n",
    "\n",
    "for entry in dataloaders['train']:\n",
    "    patient = entry[0]\n",
    "    time_series = patient[2].to(device)\n",
    "    lengths = [len(seq) for seq in time_series]\n",
    "    embed = autoencoder.encode(time_series, lengths)\n",
    "    embed_list_train.extend(embed.to(\"cpu\"))\n",
    "    \n",
    "    label = entry[1]\n",
    "    label = (label[0].to(device), label[1].to(device))\n",
    "    current_regression_label = label[0].to(device)\n",
    "    current_classification_label = label[1].to(device)\n",
    "\n",
    "    # Add each label to list keeping track of all entries.\n",
    "    regression_labels_train.extend(current_regression_label.to(\"cpu\"))\n",
    "    classification_labels_train.extend(current_classification_label.to(\"cpu\"))\n",
    "    \n",
    "\n",
    "embed_list_test = []\n",
    "regression_labels_test = []\n",
    "classification_labels_test = []\n",
    "\n",
    "for entry in dataloaders['test']:\n",
    "    patient = entry[0]\n",
    "    time_series = patient[2].to(device)\n",
    "    lengths = [len(seq) for seq in time_series]\n",
    "    embed = autoencoder.encode(time_series, lengths)\n",
    "    embed_list_test.extend(embed.to(\"cpu\"))\n",
    "    \n",
    "    label = entry[1]\n",
    "    label = (label[0].to(device), label[1].to(device))\n",
    "    current_regression_label = label[0].to(device)\n",
    "    current_classification_label = label[1].to(device)\n",
    "\n",
    "    # Add each label to list keeping track of all entries.\n",
    "    regression_labels_test.extend(current_regression_label.to(\"cpu\"))\n",
    "    classification_labels_test.extend(current_classification_label.to(\"cpu\"))\n",
    "    \n",
    "print(\"Train:\")\n",
    "print(len(embed_list_train))\n",
    "print(len(regression_labels_train))\n",
    "print(len(classification_labels_train))\n",
    "print(\"Test:\")\n",
    "print(len(embed_list_test))\n",
    "print(len(regression_labels_test))\n",
    "print(len(classification_labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_list_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_labels_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_list_train = np.array([x.numpy() for x in embed_list_train])\n",
    "regression_labels_train = np.array([x.numpy() for x in regression_labels_train])\n",
    "classification_labels_train = np.array([x.numpy() for x in classification_labels_train])\n",
    "embed_list_test = np.array([x.numpy() for x in embed_list_test])\n",
    "regression_labels_test = np.array([x.numpy() for x in regression_labels_test])\n",
    "classification_labels_test = np.array([x.numpy() for x in classification_labels_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07176148",
   "metadata": {},
   "source": [
    "### Logistic & SVM Models for Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fd63bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building the logistic regression model \n",
    "lrclf = LogisticRegression() \n",
    "lrclf.fit(embed_list_train, classification_labels_train) \n",
    "  \n",
    "# Storing the predictions of the linear model \n",
    "y_pred_lrclf = lrclf.predict(embed_list_test) \n",
    "  \n",
    "# Evaluating the performance of the linear model \n",
    "print('Accuracy : '+str(accuracy_score(classification_labels_test, y_pred_lrclf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d76be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the SVM model \n",
    "svmclf = SVC() \n",
    "svmclf.fit(embed_list_train, classification_labels_train) \n",
    "  \n",
    "# Storing the predictions of the non-linear model \n",
    "y_pred_svmclf = svmclf.predict(embed_list_test) \n",
    "  \n",
    "# Evaluating the performance of the non-linear model \n",
    "print('Accuracy : '+str(accuracy_score(classification_labels_test, y_pred_svmclf))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a59560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = SVR()\n",
    "# fit model on the training dataset\n",
    "model.fit(embed_list_train, regression_labels_train)\n",
    "# make prediction on test set\n",
    "yhat = model.predict(embed_list_test)\n",
    "# invert transforms so we can calculate errors\n",
    "# yhat = yhat.reshape((len(yhat), 1))\n",
    "# yhat = trans_out.inverse_transform(yhat)\n",
    "# y_test = trans_out.inverse_transform(regression_labels_test)\n",
    "# calculate error\n",
    "score = mean_absolute_error(regression_labels_test, yhat)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db1618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
